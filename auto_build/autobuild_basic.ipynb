{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild\n",
    "By: [Linxin Song](https://linxins97.github.io/), [Jieyu Zhang](https://jieyuz2.github.io/)\n",
    "Reference: [Agent AutoBuild](https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/)\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we introduce a new class, `AgentBuilder`, to help user build an automatic task solving process powered by multi-agent system. Specifically, in `build()`, we prompt a LLM to create multiple participant agent and initialize a group chat, and specify whether this task need programming to solve. AgentBuilder also support open-source LLMs by [vLLM](https://docs.vllm.ai/en/latest/index.html) and [Fastchat](https://github.com/lm-sys/FastChat). Check the supported model list [here](https://docs.vllm.ai/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement\n",
    "\n",
    "AutoBuild require `pyautogen[autobuild]`, which can be installed by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen[autobuild] in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: diskcache in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (5.6.3)\n",
      "Requirement already satisfied: docker in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (7.1.0)\n",
      "Requirement already satisfied: flaml in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.3 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (1.45.0)\n",
      "Requirement already satisfied: packaging in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (24.1)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (2.9.1)\n",
      "Requirement already satisfied: python-dotenv in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (1.0.1)\n",
      "Requirement already satisfied: termcolor in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (2.4.0)\n",
      "Requirement already satisfied: tiktoken in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (0.7.0)\n",
      "Collecting chromadb (from pyautogen[autobuild])\n",
      "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: huggingface-hub in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (0.24.7)\n",
      "Collecting pysqlite3 (from pyautogen[autobuild])\n",
      "  Downloading pysqlite3-0.5.3.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pyautogen[autobuild]) (3.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from openai>=1.3->pyautogen[autobuild]) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen[autobuild]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen[autobuild]) (2.23.3)\n",
      "Collecting build>=1.0.3 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading build-1.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading chroma_hnswlib-0.7.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from chromadb->pyautogen[autobuild]) (0.19.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb->pyautogen[autobuild])\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading grpcio-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from chromadb->pyautogen[autobuild]) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from chromadb->pyautogen[autobuild]) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb->pyautogen[autobuild])\n",
      "  Downloading mmh3-4.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from chromadb->pyautogen[autobuild]) (3.10.7)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from docker->pyautogen[autobuild]) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from docker->pyautogen[autobuild]) (2.2.3)\n",
      "Requirement already satisfied: filelock in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from huggingface-hub->pyautogen[autobuild]) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from huggingface-hub->pyautogen[autobuild]) (2024.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sentence-transformers->pyautogen[autobuild]) (4.44.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sentence-transformers->pyautogen[autobuild]) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sentence-transformers->pyautogen[autobuild]) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sentence-transformers->pyautogen[autobuild]) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sentence-transformers->pyautogen[autobuild]) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from tiktoken->pyautogen[autobuild]) (2024.9.11)\n",
      "Requirement already satisfied: idna>=2.8 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen[autobuild]) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen[autobuild]) (1.2.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from build>=1.0.3->chromadb->pyautogen[autobuild]) (8.5.0)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb->pyautogen[autobuild])\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb->pyautogen[autobuild])\n",
      "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen[autobuild]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen[autobuild]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen[autobuild]) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild]) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild]) (2.9.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading google_auth-2.34.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->pyautogen[autobuild])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb->pyautogen[autobuild])\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb->pyautogen[autobuild])\n",
      "  Downloading protobuf-5.28.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[autobuild]) (1.13.2)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb->pyautogen[autobuild])\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb->pyautogen[autobuild])\n",
      "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild])\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild]) (72.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild]) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[autobuild])\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->pyautogen[autobuild]) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from requests>=2.26.0->docker->pyautogen[autobuild]) (3.3.2)\n",
      "Requirement already satisfied: networkx in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (12.6.68)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers->pyautogen[autobuild]) (0.4.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from typer>=0.9.0->chromadb->pyautogen[autobuild]) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.9.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading uvloop-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading watchfiles-0.24.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[autobuild])\n",
      "  Downloading websockets-13.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from importlib-resources->chromadb->pyautogen[autobuild]) (3.20.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from scikit-learn->sentence-transformers->pyautogen[autobuild]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from scikit-learn->sentence-transformers->pyautogen[autobuild]) (3.5.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.9.0->chromadb->pyautogen[autobuild])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb->pyautogen[autobuild]) (2.18.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->pyautogen[autobuild])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->pyautogen[autobuild]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/steven/miniconda3/envs/llm_compilation/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->pyautogen[autobuild]) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb->pyautogen[autobuild])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[autobuild])\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "Downloading build-1.2.2-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n",
      "Downloading grpcio-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "Downloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
      "Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Downloading uvloop-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-13.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pysqlite3, pypika\n",
      "  Building wheel for pysqlite3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pysqlite3: filename=pysqlite3-0.5.3-cp39-cp39-linux_x86_64.whl size=47886 sha256=27f7ada787a5459a57eb6904c01984b5da60d79dc3b52c4503dc6851e0b25fcb\n",
      "  Stored in directory: /home/steven/.cache/pip/wheels/22/25/7b/395a4aa7e1953fc9441c04bca1f325e87a143b7070434506a0\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=dd6f067a097646d4a21a1f41198e2fba85ea47f027f5f27778218c71ab80f5e1\n",
      "  Stored in directory: /home/steven/.cache/pip/wheels/f7/02/64/d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pysqlite3 pypika\n",
      "Installing collected packages: pysqlite3, pypika, monotonic, mmh3, flatbuffers, websockets, websocket-client, uvloop, uvicorn, tomli, shellingham, pyproject_hooks, pyasn1, protobuf, overrides, opentelemetry-util-http, oauthlib, mdurl, importlib-resources, importlib-metadata, humanfriendly, httptools, grpcio, deprecated, chroma-hnswlib, cachetools, bcrypt, asgiref, watchfiles, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, opentelemetry-api, markdown-it-py, googleapis-common-protos, coloredlogs, build, rich, opentelemetry-semantic-conventions, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, onnxruntime, google-auth, fastapi, typer, opentelemetry-sdk, opentelemetry-instrumentation-asgi, kubernetes, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.5.0\n",
      "    Uninstalling importlib_metadata-8.5.0:\n",
      "      Successfully uninstalled importlib_metadata-8.5.0\n",
      "Successfully installed asgiref-3.8.1 bcrypt-4.2.0 build-1.2.2 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.114.2 flatbuffers-24.3.25 google-auth-2.34.0 googleapis-common-protos-1.65.0 grpcio-1.66.1 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-8.4.0 importlib-resources-6.4.5 kubernetes-30.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.6.6 protobuf-4.25.4 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.1.0 pysqlite3-0.5.3 requests-oauthlib-2.0.0 rich-13.8.1 rsa-4.9 shellingham-1.5.4 starlette-0.38.5 tomli-2.0.1 typer-0.12.5 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websocket-client-1.8.0 websockets-13.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyautogen[autobuild]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration and some useful functions\n",
    "Prepare a `config_file_or_env` for assistant agent to limit the choice of LLM you want to use in this task. This config can be a path of json file or a name of environment variable. A `default_llm_config` is also required for initialize the specific config of LLMs like seed, temperature, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2505f029423b21ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:07:41.225066900Z",
     "start_time": "2024-06-09T15:07:40.443327100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "config_file_or_env = \"OAI_CONFIG_LIST.json\"\n",
    "llm_config = {'model': 'gpt-4o',\n",
    "                'api_key': \"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\",\n",
    "                'temperature': 0.2}\n",
    "\n",
    "\n",
    "def start_task(execution_task: str, agent_list: list, coding=True):\n",
    "    group_chat = autogen.GroupChat(\n",
    "        agents=agent_list,\n",
    "        messages=[],\n",
    "        max_round=12,\n",
    "        allow_repeat_speaker=agent_list[:-1] if coding is True else agent_list,\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=group_chat,\n",
    "        llm_config={ **llm_config},\n",
    "    )\n",
    "    agent_list[0].initiate_chat(manager, message=execution_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder\n",
    "Create a `AgentBuilder` with the specified `config_path_or_env`. AgentBuilder will use `gpt-4` in default to complete the whole process, you can specify the `builder_model` and `agent_model` to other OpenAI model to match your task. \n",
    "You can also specify an open-source LLM supporting by vLLM and FastChat, see blog for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:07:54.256131900Z",
     "start_time": "2024-06-09T15:07:54.236884400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 09-27 15:13:55] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "builder = AgentBuilder(\n",
    "    config_file_or_env=config_file_or_env, builder_model=[\"gpt-4o\"], agent_model=[\"gpt-4o\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. Building task will help build manager (a LLM) decide what agents should be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68315f6ec912c58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:07:57.283793900Z",
     "start_time": "2024-06-09T15:07:57.274718Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building_task = \"Generate agents that can take in a pdf file, which is a chapter of a book called commonsense reasoning, and extract all the examples in it. The examples can have natural language descriptions as well as corresponding logical expressions. I want examples only in natural language. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:08:45.446026500Z",
     "start_time": "2024-06-09T15:07:58.296262400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m==> Generating agents...\u001b[0m\n",
      "['NLP_Expert', 'DataExtraction_Expert', 'PDFProcessing_Expert', 'TextAnalysis_Expert', 'LogicalExpression_Expert'] are generated.\n",
      "\u001b[32m==> Generating system message...\u001b[0m\n",
      "Preparing system message for NLP_Expert\n",
      "Preparing system message for DataExtraction_Expert\n",
      "Preparing system message for PDFProcessing_Expert\n",
      "Preparing system message for TextAnalysis_Expert\n",
      "Preparing system message for LogicalExpression_Expert\n",
      "\u001b[32m==> Generating description...\u001b[0m\n",
      "Preparing description for NLP_Expert\n",
      "Preparing description for DataExtraction_Expert\n",
      "Preparing description for PDFProcessing_Expert\n",
      "Preparing description for TextAnalysis_Expert\n",
      "Preparing description for LogicalExpression_Expert\n",
      "\u001b[32m==> Creating agents...\u001b[0m\n",
      "Creating agent NLP_Expert...\n",
      "[autogen.oai.client: 09-27 15:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "Creating agent DataExtraction_Expert...\n",
      "[autogen.oai.client: 09-27 15:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "Creating agent PDFProcessing_Expert...\n",
      "[autogen.oai.client: 09-27 15:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "Creating agent TextAnalysis_Expert...\n",
      "[autogen.oai.client: 09-27 15:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "Creating agent LogicalExpression_Expert...\n",
      "[autogen.oai.client: 09-27 15:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "Adding user console proxy...\n",
      "\u001b[32mBuilding config saved to ./save_config_ae435036a620d2111ac1093c2e371b6b.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent_list, agent_configs = builder.build(building_task, llm_config)\n",
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Let agents generated in `build()` to complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:10:37.719729400Z",
     "start_time": "2024-06-09T15:08:58.365570500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 09-16 17:31:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[33mGo_Compiler_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "Clone and build this repository at https://github.com/netdata/netdata from source under current directory; the sudo password is 'Hacking!', always take a look at the README.md file if it is available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "There is no code from the last 1 message for me to execute. Group chat manager should let other participants to continue the conversation. If the group chat manager want to end the conversation, you should let other participant reply me only with \"TERMINATE\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Local_Go_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mLocal_Go_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure, let's start by cloning the repository and checking the README.md file for any specific build instructions.\n",
      "\n",
      "```sh\n",
      "# Clone the repository\n",
      "git clone https://github.com/netdata/netdata.git\n",
      "\n",
      "# Change to the repository directory\n",
      "cd netdata\n",
      "\n",
      "# Display the README.md file\n",
      "cat README.md\n",
      "```\n",
      "\n",
      "Let's execute these commands to understand the build instructions provided in the README.md file.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "\n",
      "<p align=\"center\">\n",
      "<a href=\"https://www.netdata.cloud#gh-light-mode-only\">\n",
      "  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png\" alt=\"Netdata\" width=\"300\"/>\n",
      "</a>\n",
      "<a href=\"https://www.netdata.cloud#gh-dark-mode-only\">\n",
      "  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png\" alt=\"Netdata\" width=\"300\"/>\n",
      "</a>\n",
      "</p>\n",
      "<h3 align=\"center\">Monitor your servers, containers, and applications,<br/>in high-resolution and in real-time.</h3>\n",
      "\n",
      "<br />\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://github.com/netdata/netdata/\"><img src=\"https://img.shields.io/github/stars/netdata/netdata?style=social\" alt=\"GitHub Stars\"></a>\n",
      "  <br />\n",
      "  <a href=\"https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge\"><img src=\"https://img.shields.io/badge/Live%20Demo-green\" alt=\"Live Demo\"></a>\n",
      "  <a href=\"https://github.com/netdata/netdata/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata.svg\" alt=\"Latest release\"></a>\n",
      "  <a href=\"https://github.com/netdata/netdata-nightlies/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata-nightlies.svg\" alt=\"Latest nightly build\"></a>\n",
      "  <br/>\n",
      "  <a href=\"https://community.netdata.cloud\"><img alt=\"Discourse topics\" src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum\"></a>\n",
      "  <a href=\"https://github.com/netdata/netdata/discussions\"><img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions\"></a>\n",
      "</p>\n",
      "\n",
      "<p align=\"center\"><b>Visit the <a href=\"https://www.netdata.cloud\">Project's Home Page</a></b></p>\n",
      "\n",
      "<hr class=\"solid\">\n",
      "\n",
      "MENU: **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#license)**\n",
      "\n",
      "> **Important** :bulb:<br/>\n",
      "> People get addicted to Netdata. Once you use it on your systems, **there's no going back!**<br/>\n",
      "\n",
      "**Netdata** is a high-performance, cloud-native, and on-premises observability platform designed to monitor metrics and logs with unparalleled efficiency. It delivers a simpler, faster, and significantly easier approach to real-time, low-latency monitoring for systems, containers, and applications. Netdata requires **zero-configuration** to get started, offering a powerful and comprehensive monitoring experience, out of the box.\n",
      "\n",
      "Netdata is also known for its **cost-efficient, distributed design**. Unlike traditional monitoring solutions that centralize data, **Netdata distributes the code**. Instead of funneling all data into a few central databases, Netdata processes data at the edge, keeping it close to the source. The smart open-source Netdata Agent acts as a distributed database, enabling the construction of complex observability pipelines with modular, Lego-like simplicity.\n",
      "\n",
      "Netdata provides **A.I. insights** for all monitored data, training machine learning models directly at the edge. This allows for fully automated and unsupervised anomaly detection, and with its intuitive APIs and UIs, users can quickly perform root cause analysis and troubleshoot issues, identifying correlations and gaining deeper insights into their infrastructure.\n",
      "\n",
      "### The Netdata Ecosystem\n",
      "\n",
      "Netdata is built on three core components:\n",
      "\n",
      "1.  **Netdata Agent** (usually called just \"Netdata\"): This open-source component is the heart of the Netdata ecosystem, handling data collection, storage (embedded database), querying, machine learning, exporting, and alerting of observability data. All observability data and features a Netdata ecosystem offers, are managed by the Netdata Agent. It runs in physical and virtual servers, cloud environments, Kubernetes clusters, and edge/IoT devices and is carefully optimized to have _**zero impact**_ on production systems and applications.\n",
      "    \n",
      "    <a href=\"https://www.gnu.org/licenses/gpl-3.0\"><img src=\"https://img.shields.io/badge/Netdata%20Agent%20License-GPL%20v3%2B-blue.svg\" alt=\"Netdata Agent License: GPL v3+\"></a> <a href=\"https://bestpractices.coreinfrastructure.org/projects/2231\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/2231/badge\" alt=\"CII Best Practices\"></a> <a href=\"https://scan.coverity.com/projects/netdata-netdata?tab=overview\"><img alt=\"Coverity Scan\" src=\"https://img.shields.io/coverity/scan/netdata\"></a>\n",
      "    \n",
      "2.  **Netdata Cloud**: Enhancing the Netdata Agent, Netdata Cloud offers enterprise features such as user management, role-based access control, horizontal scalability, alert and notification management, access from anywhere, and more.  Netdata Cloud does **not** centralize or store observability data.\n",
      "    \n",
      "    _Netdata Cloud is a commercial product, available as an on-premises installation, or a SaaS solution, with a free community tier._\n",
      "    \n",
      "3.  **Netdata UI**: The user interface that powers all dashboards, data visualization, and configuration.\n",
      "    \n",
      "    _While closed-source, it is free to use with both Netdata Agents and Netdata Cloud, via their public APIs. It is included in the binary packages offered by Netdata and its latest version is publicly available via a CDN._\n",
      "\n",
      "    <a href=\"https://github.com/netdata/netdata/blob/master/src/web/gui/v2/LICENSE.md\"><img src=\"https://img.shields.io/badge/Netdata%20UI%20License%20-NCUL1-blue.svg\" alt=\"Netdata UI License: NCUL1\"></a>\n",
      "    \n",
      "Netdata scales effortlessly from a single server to thousands, even in complex, multi-cloud or hybrid environments, with the ability to retain data for years.\n",
      "\n",
      "### Key characteristics of the Netdata Agent\n",
      "\n",
      "- :boom: **Collects data from 800+ integrations**<br/>\n",
      "  Operating system metrics, container metrics, virtual machines, hardware sensors, applications metrics, OpenMetrics exporters, StatsD, and logs. OpenTelemetry is on its way to be included (currently being developed)...\n",
      "  \n",
      "- :muscle: **Real-Time, Low-Latency, High-Resolution**<br/>\n",
      "  All data are collected per second and are made available on the APIs for visualization, immediately after data collection (1-second latency, data collection to visualization).\n",
      "\n",
      "- :face_in_clouds: **AI across the board**<br/>\n",
      "  Trains multiple Machine-Learning (ML) models at the edge, for each metric collected and uses AI to detect anomalies based on the past behavior of each metric.\n",
      "\n",
      "- :scroll:  **systemd-journald Logs**<br/>\n",
      "  Includes tools to efficiently convert plain text log (text, csv, logfmt, json) files to structured systemd-journald entries (`log2journal`, `systemd-cat-native`) and queries systemd-journal files directly enabling powerful logs visualization dashboards. The Netdata Agents eliminate the need to centralize logs and provide all the functions to work with logs directly at the edge.\n",
      "\n",
      "- :star: **Lego like, Observability Pipelines**<br/>\n",
      "  Netdata Agents can be linked to together (in parent-child relationships), to build observability centralization points within your infrastructure, allowing you to control data replication and retention at multiple levels.\n",
      "\n",
      "- :fire: **Fully Automated Powerful Visualization**<br/>\n",
      "  Using the NIDL (Nodes, Instances, Dimensions & Labels) data model, the Netdata Agent enables the creation of fully automated dashboards, providing corellated visualization of all metrics, allowing you to understand any dataset at first sight, but also to filter, slice and dice the data directly on the dashboards, without the need to learn a query language.\n",
      "\n",
      "  <small>Note: the Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud.</small>\n",
      "\n",
      "- :bell: **Out of box Alerts**<br/>\n",
      "  Comes with hundreds of alerts out of the box to detect common issues and pitfalls, revealing issues that can easily go unnoticed. It supports several notification methods to let you know when your attention is needed.\n",
      "\n",
      "- :sunglasses: **Low Maintenance**<br/>\n",
      "  Fully automated in every aspect: automated dashboards, out-of-the-box alerts, auto-detection and auto-discovery of metrics, zero-touch machine-learning, easy scalability and high availability, and CI/CD friendly.\n",
      "\n",
      "- :star: **Open and Extensible**<br/>\n",
      "  Netdata is a modular platform that can be extended in all possible ways, and it also integrates nicely with other monitoring solutions.\n",
      "\n",
      "\n",
      "### What can be monitored with the Netdata Agent\n",
      "\n",
      "Netdata monitors all the following:\n",
      "\n",
      "|                                                                                                   Component |              Linux               | FreeBSD | macOS |             Windows*             |\n",
      "|------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------:|:-----:|:--------------------------------:|\n",
      "|                             **System Resources**<small><br/>CPU, Memory and system shared resources</small> |               Full               |   Yes   |  Yes  |               Yes                |\n",
      "|                                **Storage**<small><br/>Disks, Mount points, Filesystems, RAID arrays</small> |               Full               |  Basic  | Basic |              Basic               |\n",
      "|                                 **Network**<small><br/>Network Interfaces, Protocols, Firewall, etc</small> |               Full               |  Basic  | Basic |              Basic               |\n",
      "|                        **Hardware & Sensors**<small><br/>Fans, Temperatures, Controllers, GPUs, etc</small> |               Full               |  Some   | Some  |               Some               |\n",
      "|                                       **O/S Services**<small><br/>Resources, Performance and Status</small> | Yes<small><br/>`systemd`</small> |    -    |   -   |              Basic               |\n",
      "|                                                                                                    **Logs** | Yes<small><br/>`systemd`-journal |    -    |   -   |                -                 |\n",
      "|                                      **Processes**<small><br/>Resources, Performance, OOM, and more</small> |               Yes                |   Yes   |  Yes  |               Yes                |\n",
      "|                                 **Network Connections**<small><br/>Live TCP and UDP sockets per PID</small> |               Yes                |    -    |   -   |                -                 |\n",
      "|                               **Containers**<small><br/>Docker/containerd, LXC/LXD, Kubernetes, etc</small> |               Yes                |    -    |   -   |                -                 |\n",
      "|                                 **VMs** (from the host)<small><br/>KVM, qemu, libvirt, Proxmox, etc</small> | Yes<small><br/>`cgroups`</small> |    -    |   -   | Yes<small><br/>`Hyper-V`</small> |\n",
      "|                       **Synthetic Checks**<small><br/>Test APIs, TCP ports, Ping, Certificates, etc</small> |               Yes                |   Yes   |  Yes  |               Yes                |\n",
      "| **Packaged Applications**<small><br/>nginx, apache, postgres, redis, mongodb,<br/>and hundreds more</small> |               Yes                |   Yes   |  Yes  |               Yes                |\n",
      "| **Cloud Provider Infrastructure**<small><br/>AWS, GCP, Azure, and more</small> |               Yes                |   Yes   |  Yes  |               Yes                |\n",
      "|                                              **Custom Applications**<small><br/>OpenMetrics, StatsD and soon OpenTelemetry</small> |               Yes                |   Yes   |  Yes  |               Yes                |\n",
      "\n",
      "When the Netdata Agent runs on Linux, it monitors every kernel feature available, providing full coverage of all kernel technologies that can be monitored.\n",
      "\n",
      "The Netdata Agent also provides full **enterprise hardware** coverage, monitoring all components that provide hardware error reporting, like PCI AER, RAM EDAC, IPMI, S.M.A.R.T., NVMe, Fans, Power, Voltages, and more.\n",
      "\n",
      "<small> * The Netdata Agent runs on Linux, FreeBSD and macOS. For Windows, we currently rely on Windows Exporter (so a Netdata running on Linux, FreeBSD or macOS is required, next to the monitored Windows servers). However, a Windows version of the Netdata Agent is at its final state for release.</small>\n",
      "\n",
      "---\n",
      "\n",
      "### :star: Netdata is the most energy-efficient monitoring tool :star:\n",
      "\n",
      "<p align=\"center\">\n",
      "<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only\">\n",
      "  <img src=\"https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0\" alt=\"Energy Efficiency\" width=\"1000\"/>\n",
      "</a>\n",
      "<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only\">\n",
      "  <img src=\"https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219\" alt=\"Energy efficiency\" width=\"1000\"/>\n",
      "</a>\n",
      "</p>\n",
      "\n",
      "Dec 11, 2023: [University of Amsterdam published a study](https://twitter.com/IMalavolta/status/1734208439096676680) related to the impact of monitoring tools for Docker based systems, aiming to answer 2 questions:\n",
      "\n",
      "1. **The impact of monitoring on the energy efficiency of Docker-based systems**\n",
      "2. **The impact of monitoring on Docker-based systems?**\n",
      "\n",
      "- 🚀 Netdata excels in energy efficiency: **\"... Netdata being the most energy-efficient tool ...\"**, as the study says.\n",
      "- 🚀 Netdata excels in CPU Usage, RAM Usage and Execution Time, and has a similar impact in Network Traffic as Prometheus.\n",
      "\n",
      "The study did not normalize the results based on the number of metrics collected. Given that Netdata usually collects significantly more metrics than the other tools, Netdata managed to outperform the other tools, while ingesting a much higher number of metrics. [Read the full study here](https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf).\n",
      "\n",
      "---\n",
      "\n",
      "### Netdata vs Prometheus\n",
      "\n",
      "<p align=\"center\">\n",
      "<a href=\"https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis#gh-light-mode-only\">\n",
      "  <img src=\"https://github.com/netdata/netdata/assets/139226121/6c21ae39-8656-45c3-bc85-4b012679d2bb\" alt=\"Netdata\" width=\"1000\"/>\n",
      "</a>\n",
      "<a href=\"https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis#gh-dark-mode-only\">\n",
      "  <img src=\"https://github.com/netdata/netdata/assets/139226121/f2dbde46-d3dd-4807-bd34-966da4d0ec22\" alt=\"Netdata\" width=\"1000\"/>\n",
      "</a>\n",
      "</p>\n",
      "\n",
      "On the same workload, Netdata uses **35% less CPU**, **49% less RAM**, **12% less bandwidth**, **98% less disk I/O**, and is **75% more disk space efficient** on high resolution metrics storage, while providing more than a year of overall retention on the same disk footprint Prometheus offers 7 days of retention. [Read the full analysis in our blog](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/).\n",
      "\n",
      "---\n",
      "\n",
      "&nbsp;<br/>\n",
      "<p align=\"center\">\n",
      "  <img src=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg#gh-dark-mode-only\" alt=\"CNCF\" width=\"300\">\n",
      "  <img src=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/black/cncf-black.svg#gh-light-mode-only\" alt=\"CNCF\" width=\"300\">\n",
      "  <br />\n",
      "  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF)<br />\n",
      "  &nbsp;<br/>\n",
      "  ...and due to your love :heart:, it is one of the most :star:'d projects in the <a href=\"https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata\">CNCF landscape</a>!\n",
      "</p>\n",
      "&nbsp;<br/>\n",
      "\n",
      "<hr class=\"solid\">\n",
      "\n",
      "<p align=\"center\">\n",
      "  <b>Below is an animated image, but you can see Netdata live!</b><br/>\n",
      "\t<a href=\"https://frankfurt.netdata.rocks\"><b>FRANKFURT</b></a> |\n",
      "\t<a href=\"https://newyork.netdata.rocks\"><b>NEWYORK</b></a> |\n",
      "\t<a href=\"https://atlanta.netdata.rocks\"><b>ATLANTA</b></a> |\n",
      "\t<a href=\"https://sanfrancisco.netdata.rocks\"><b>SANFRANCISCO</b></a> |\n",
      "\t<a href=\"https://toronto.netdata.rocks\"><b>TORONTO</b></a> |\n",
      "\t<a href=\"https://singapore.netdata.rocks\"><b>SINGAPORE</b></a> |\n",
      "\t<a href=\"https://bangalore.netdata.rocks\"><b>BANGALORE</b></a>\n",
      "  <br/>\n",
      "  \t<i>They are clustered Netdata Agent Parents. They all have the same data. <b>Select the one closer to you.</b></i>\n",
      "\t<br/>\n",
      "\t<i>All these run with the default configuration. We only clustered them to have multi-node dashboards.</i>\n",
      "\t<br/><small>Note: These demos include the Netdata UI,<br/>which while being closed-source, is free to use with Netdata Agents and Netdata Cloud.</small>\n",
      "</p>\n",
      "\n",
      "![Netdata Agent](https://github.com/netdata/netdata/assets/2662304/af4caa23-19be-46ef-9779-8fdad8d99d2a)\n",
      "\n",
      "<hr class=\"solid\">\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&label=user%20base&units=M&value_color=blue&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"User base\"></a>\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&label=servers%20monitored&units=M&divide=1000000&value_color=orange&precision=2&options=unaligned&tier=1&v44\" alt=\"Servers monitored\"></a>\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&label=sessions%20served&units=M&value_color=yellowgreen&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"Sessions served\"></a>\n",
      "  <a href=\"https://hub.docker.com/r/netdata/netdata\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000000&precision=1&units=M&label=docker+hub+pulls&options=unaligned&tier=1&v44\" alt=\"Docker Hub pulls\"></a>\n",
      "  <br />\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&after=-86400&options=unaligned&group=incremental-sum&label=new%20users%20today&units=null&value_color=blue&precision=0&options=unaligned&tier=1&v44\" alt=\"New users today\"></a>\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&group=incremental-sum&after=-86400&options=unaligned&label=servers%20added%20today&units=null&value_color=orange&precision=0&tier=1&v44\" alt=\"New machines today\"></a>\n",
      "  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&after=-86400&group=incremental-sum&options=unaligned&label=sessions%20served%20today&units=null&value_color=yellowgreen&precision=0&tier=1&v44\" alt=\"Sessions today\"></a>\n",
      "  <a href=\"https://hub.docker.com/r/netdata/netdata\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000&precision=1&units=k&label=docker+hub+pulls&after=-86400&group=incremental-sum&label=docker%20hub%20pulls%20today&options=unaligned&tier=1&v44\" alt=\"Docker Hub pulls today\"></a>\n",
      "</p>\n",
      "\n",
      "### 1. **Install Netdata everywhere** :v:\n",
      "   \n",
      "   Netdata can be installed on all Linux, macOS, FreeBSD (and soon on Windows) systems. We provide binary packages for the most popular operating systems and package managers.\n",
      "\n",
      "   - Install on [Ubuntu, Debian CentOS, Fedora, Suse, Red Hat, Arch, Alpine, Gentoo, even BusyBox](https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems).\n",
      "   - Install with [Docker](/packaging/docker/README.md).<br/>\n",
      "     Netdata is a [Verified Publisher on DockerHub](https://hub.docker.com/r/netdata/netdata) and our users enjoy free unlimited DockerHub pulls :heart_eyes:.\n",
      "   - Install on [macOS](https://learn.netdata.cloud/docs/installing/macos) :metal:.\n",
      "   - Install on [FreeBSD](https://learn.netdata.cloud/docs/installing/freebsd) and [pfSense](https://learn.netdata.cloud/docs/installing/pfsense).\n",
      "   - Install [from source](https://learn.netdata.cloud/docs/installing/build-the-netdata-agent-yourself/compile-from-source-code) ![github downloads](https://img.shields.io/github/downloads/netdata/netdata/total?color=success&logo=github)\n",
      "   - For Kubernetes deployments [check here](https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes/).\n",
      "\n",
      "  Check also the [Netdata Deployment Guides](https://learn.netdata.cloud/docs/deployment-guides/) to decide how to deploy it in your infrastructure.\n",
      "\n",
      "  By default, you will have immediately available a local dashboard. Netdata starts a web server for its dashboard at port `19999`. Open up your web browser of choice and\n",
      "navigate to `http://NODE:19999`, replacing `NODE` with the IP address or hostname of your Agent. If installed on localhost, you can access it through `http://localhost:19999`. \n",
      "\n",
      "  _Note: the binary packages we provide, install Netdata UI automatically. Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud._\n",
      "\n",
      "### 2. **Configure Collectors** :boom:\n",
      "\n",
      "   Netdata auto-detects and auto-discovers most operating system data sources and applications. However, many data sources require some manual configuration, usually to allow Netdata to get access to the metrics.\n",
      "   \n",
      "   - For a detailed list of the 800+ collectors available, check [this guide](https://learn.netdata.cloud/docs/data-collection/).\n",
      "   - To monitor Windows servers and applications use [this guide](https://learn.netdata.cloud/docs/data-collection/monitor-anything/system-metrics/windows-machines).<br/><small>Note that Netdata on Windows is at its final release stage, so at the next Netdata release Netdata will natively support Windows.</small>\n",
      "   - To monitor SNMP devices check [this guide](https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp).\n",
      "\n",
      "### 3. **Configure Alert Notifications** :bell:\n",
      "\n",
      "   Netdata comes with hundreds of pre-configured alerts, that automatically check your metrics, immediately after they start getting collected.\n",
      "\n",
      "   Netdata can dispatch alert notifications to multiple third party systems, including: `email`, `Alerta`, `AWS SNS`, `Discord`, `Dynatrace`, `flock`, `gotify`, `IRC`, `Matrix`, `MessageBird`, `Microsoft Teams`, `ntfy`, `OPSgenie`, `PagerDuty`, `Prowl`, `PushBullet`, `PushOver`, `RocketChat`, `Slack`, `SMS tools`, `Syslog`, `Telegram`, `Twilio`.\n",
      "\n",
      "   By default, Netdata will send e-mail notifications, if there is a configured MTA on the system. \n",
      "\n",
      "### 4. **Configure Netdata Parents** :family:\n",
      "\n",
      "   Optionally, configure one or more Netdata Parents. A Netdata Parent is a Netdata Agent that has been configured to accept [streaming connections](https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference) from other Netdata agents.\n",
      "   \n",
      "   Netdata Parents provide:\n",
      "\n",
      "   - **Infrastructure level dashboards, at `http://parent.server.ip:19999/`.**<br/>\n",
      "   \n",
      "     Each Netdata Agent has an API listening at the TCP port 19999 of each server.\n",
      "     When you hit that port with a web browser (e.g. `http://server.ip:19999/`), the Netdata Agent UI is presented.\n",
      "     When the Netdata Agent is also a Parent, the UI of the Parent includes data for all nodes that stream metrics to that Parent.\n",
      "     \n",
      "   - **Increased retention for all metrics of all your nodes.**<br/>\n",
      "   \n",
      "     Each Netdata Agent maintains each own database of metrics. But Parents can be given additional resources to maintain a much longer database than\n",
      "     individual Netdata Agents.\n",
      "    \n",
      "   - **Central configuration of alerts and dispatch of notifications.**<br/>\n",
      "   \n",
      "     Using Netdata Parents, all the alert notifications integrations can be configured only once, at the Parent and they can be disabled at the Netdata Agents.\n",
      "\n",
      "   You can also use Netdata Parents to:\n",
      "\n",
      "   - Offload your production systems (the parents run ML, alerts, queries, etc. for all their children)\n",
      "   - Secure your production systems (the parents accept user connections, for all their children)\n",
      "\n",
      "### 5. **Connect to Netdata Cloud** :cloud:\n",
      "\n",
      "   [Sign-in](https://app.netdata.cloud/sign-in) to [Netdata Cloud](https://www.netdata.cloud/) and claim your Netdata Agents and Parents.\n",
      "   If you connect your Netdata Parents, there is no need to connect your Netdata Agents. They will be connected via the Parents.\n",
      "   \n",
      "   When your Netdata nodes are connected to Netdata Cloud, you can (on top of the above):\n",
      "\n",
      "   - Access your Netdata agents from anywhere\n",
      "   - Access sensitive Netdata agent features (like \"Netdata Functions\": processes, systemd-journal)\n",
      "   - Organize your infra in spaces and Rooms\n",
      "   - Create, manage, and share **custom dashboards**\n",
      "   - Invite your team and assign roles to them (Role Based Access Control - RBAC)\n",
      "   - Get infinite horizontal scalability (multiple independent Netdata Agents are viewed as one infra)\n",
      "   - Configure alerts from the UI\n",
      "   - Configure data collection from the UI\n",
      "   - Netdata Mobile App notifications\n",
      "\n",
      "   :love_you_gesture: Netdata Cloud does not prevent you from using your Netdata Agents and Parents directly, and vice versa.<br/>\n",
      "   \n",
      "   :ok_hand: Your metrics are still stored in your network when you connect your Netdata Agents and Parents to Netdata Cloud.\n",
      "\n",
      "<hr class=\"solid\">\n",
      "\n",
      "## How it works\n",
      "\n",
      "Netdata is built around a **modular metrics processing pipeline**.\n",
      "\n",
      "<details><summary>Click to see more details about this pipeline...</summary>\n",
      "&nbsp;<br/>\n",
      "\n",
      "Each Netdata Agent can perform the following functions:\n",
      "\n",
      "1. **`COLLECT` metrics from their sources**<br/>\n",
      "   Uses [internal](https://github.com/netdata/netdata/tree/master/src/collectors) and [external](https://github.com/netdata/go.d.plugin/tree/master/modules) plugins to collect data from their sources.\n",
      "\n",
      "   Netdata auto-detects and collects almost everything from the operating system: including CPU, Interrupts, Memory, Disks, Mount Points, Filesystems, Network Stack, Network Interfaces, Containers, VMs, Processes, `systemd` units, Linux Performance Metrics, Linux eBPF, Hardware Sensors, IPMI, and more.\n",
      "\n",
      "   It collects application metrics from applications: PostgreSQL, MySQL/MariaDB, Redis, MongoDB, Nginx, Apache, and hundreds more.\n",
      "\n",
      "   Netdata also collects your custom application metrics by scraping OpenMetrics exporters, or via StatsD.\n",
      "\n",
      "   It can convert web server log files to metrics and apply ML and alerts to them, in real-time.\n",
      "\n",
      "   And it also supports synthetic tests / white box tests, so you can ping servers, check API responses, or even check filesystem files and directories to generate metrics, train ML and run alerts and notifications on their status.\n",
      "   \n",
      "2. **`STORE` metrics to a database**<br/>\n",
      "   Uses database engine plugins to store the collected data, either in memory and/or on disk. We have developed our own [`dbengine`](https://github.com/netdata/netdata/tree/master/src/database/engine#readme) for storing the data in a very efficient manner, allowing Netdata to have less than 1 byte per sample on disk and amazingly fast queries.\n",
      "   \n",
      "3. **`LEARN` the behavior of metrics** (ML)<br/>\n",
      "   Trains multiple Machine-Learning (ML) models per metric to learn the behavior of each metric individually. Netdata uses the `kmeans` algorithm and creates by default a model per metric per hour, based on the values collected for that metric over the last 6 hours. The trained models are persisted to disk.\n",
      "   \n",
      "4. **`DETECT` anomalies in metrics** (ML)<br/>\n",
      "   Uses the trained machine learning (ML) models to detect outliers and mark collected samples as **anomalies**. Netdata stores anomaly information together with each sample and also streams it to Netdata Parents so that the anomaly is also available at query time for the whole retention of each metric.\n",
      "\n",
      "5. **`CHECK` metrics and trigger alert notifications**<br/>\n",
      "   Uses its configured alerts (you can configure your own) to check the metrics for common issues and uses notifications plugins to send alert notifications.\n",
      "\n",
      "6. **`STREAM` metrics to other Netdata Agents**<br/>\n",
      "   Push metrics in real-time to Netdata Parents.\n",
      "\n",
      "7. **`ARCHIVE` metrics to 3rd party databases**<br/>\n",
      "   Export metrics to industry standard time-series databases, like `Prometheus`, `InfluxDB`, `OpenTSDB`, `Graphite`, etc.\n",
      "\n",
      "8. **`QUERY` metrics and present dashboards**<br/>\n",
      "   Provide an API to query the data and present interactive dashboards to users.\n",
      "\n",
      "9. **`SCORE` metrics to reveal similarities and patterns**<br/>\n",
      "   Score the metrics according to the given criteria, to find the needle in the haystack.\n",
      "\n",
      "When using Netdata Parents, all the functions of a Netdata Agent (except data collection) can be delegated to Parents to offload production systems.\n",
      "\n",
      "The core of Netdata is developed in C. We have our own `libnetdata`, that provides:\n",
      "\n",
      "- **`DICTIONARY`**<br/>\n",
      "  A high-performance algorithm to maintain both indexed and ordered pools of structures Netdata needs. It uses JudyHS arrays for indexing, although it is modular: any hashtable or tree can be integrated into it. Despite being in C, dictionaries follow object-oriented programming principles, so there are constructors, destructors, automatic memory management, garbage collection, and more. For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/dictionary).\n",
      "  \n",
      "- **`ARAL`**<br/>\n",
      "  ARray ALlocator (ARAL) is used to minimize the system allocations made by Netdata. ARAL is optimized for maximum multithreaded performance. It also allows all structures that use it to be allocated in memory-mapped files (shared memory) instead of RAM. For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/aral).\n",
      "\n",
      "- **`PROCFILE`**<br/>\n",
      "  A high-performance `/proc` (but also any) file parser and text tokenizer. It achieves its performance by keeping files open and adjusting its buffers to read the entire file in one call (which is also required by the Linux kernel). For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/procfile).\n",
      "\n",
      "- **`STRING`**<br/>\n",
      "  A string internet mechanism, for string deduplication and indexing (using JudyHS arrays), optimized for multithreaded usage. For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/string).\n",
      "\n",
      "- **`ARL`**<br/>\n",
      "  Adaptive Resortable List (ARL), is a very fast list iterator, that keeps the expected items on the list in the same order they are found in input list. So, the first iteration is somewhat slower, but all the following iterations are perfectly aligned for best performance. For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/adaptive_resortable_list).\n",
      "\n",
      "- **`BUFFER`**<br/>\n",
      "  A flexible text buffer management system that allows Netdata to automatically handle dynamically sized text buffer allocations. The same mechanism is used for generating consistent JSON output by the Netdata APIs. For more see [here](https://github.com/netdata/netdata/tree/master/src/libnetdata/buffer).\n",
      "\n",
      "- **`SPINLOCK`**<br/>\n",
      "  Like POSIX `MUTEX` and `RWLOCK` but a lot faster, based on atomic operations, with significantly smaller memory impact, while being portable.\n",
      "\n",
      "- **`PGC`**<br/>\n",
      "  A caching layer that can be used to cache any kind of time-related data, with automatic indexing (based on a tree of JudyL arrays), memory management, evictions, flushing, pressure management. This is extensively used in `dbengine`. For more see [here](/src/database/engine/README.md).\n",
      "\n",
      "The above, and many more, allow Netdata developers to work on the application fast and with confidence. Most of the business logic in Netdata is a work of mixing the above.\n",
      "\n",
      "Netdata data collection plugins can be developed in any language. Most of our application collectors though are developed in [Go](https://github.com/netdata/go.d.plugin).\n",
      "\n",
      "</details>\n",
      "\n",
      "## FAQ\n",
      "\n",
      "### :shield: Is Netdata secure?\n",
      "\n",
      "Of course, it is! We do our best to ensure it is!\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "We understand that Netdata is a software piece that is installed on millions of production systems across the world. So, it is important for us, Netdata to be as secure as possible:\n",
      "\n",
      "  - We follow the [Open Source Security Foundation](https://bestpractices.coreinfrastructure.org/en/projects/2231) best practices.\n",
      "  - We have given great attention to detail when it comes to security design. Check out our [security design](/docs/security-and-privacy-design/README.md).\n",
      "  - Netdata is a popular open-source project and is frequently tested by many security analysts.\n",
      "  - Check also our [security policies and advisories published so far](https://github.com/netdata/netdata/security).\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :cyclone: Will Netdata consume significant resources on my servers?\n",
      "\n",
      "No. It will not! We promise this will be fast!\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Although each Netdata Agent is a complete monitoring solution packed into a single application, and despite the fact that Netdata collects **every metric every single second** and trains **multiple ML models** per metric, you will find that Netdata has amazing performance! In many cases, it outperforms other monitoring solutions that have significantly fewer features or far smaller data collection rates.\n",
      "\n",
      "This is what you should expect:\n",
      "\n",
      "  - For production systems, each Netdata Agent with default settings (everything enabled, ML, Health, DB) should consume about 5% CPU utilization of one core and about 150 MiB or RAM.\n",
      "\n",
      "    By using a Netdata parent and streaming all metrics to that parent, you can disable ML & health and use an ephemeral DB (like `alloc`) on the children, leading to utilization of about 1% CPU of a single core and 100 MiB of RAM. Of course, these depend on how many metrics are collected.\n",
      "    \n",
      "  - For Netdata Parents, for about 1 to 2 million metrics, all collected every second, we suggest a server with 16 cores and 32GB RAM. Less than half of it will be used for data collection and ML. The rest will be available for queries.\n",
      "\n",
      "Netdata has extensive internal instrumentation to help us reveal how the resources consumed are used. All these are available in the \"Netdata Monitoring\" section of the dashboard. Depending on your use case, there are many options to optimize resource consumption.\n",
      "\n",
      "Even if you need to run Netdata on extremely weak embedded or IoT systems, you will find that Netdata can be tuned to be very performant.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :scroll: How much retention can I have?\n",
      "\n",
      "As much as you need!\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Netdata supports **tiering**, to downsample past data and save disk space. With default settings, it has 3 tiers:\n",
      "\n",
      "  1. `tier 0`, with high resolution, per-second, data.\n",
      "  2. `tier 1`, mid-resolution, per minute, data.\n",
      "  3. `tier 2`, low-resolution, per hour, data.\n",
      "\n",
      "All tiers are updated in parallel during data collection. Just increase the disk space you give to Netdata to get a longer history for your metrics. Tiers are automatically chosen at query time depending on the time frame and the resolution requested.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :rocket: Does it scale? I have really a lot of servers!\n",
      "\n",
      "Netdata is designed to scale and can handle large volumes of data.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "Netdata is a distributed monitoring solution. You can scale it to infinity by spreading Netdata Agents across your infrastructure.\n",
      "\n",
      "With the streaming feature of the Agent, we can support monitoring ephemeral servers but also allow the creation of \"monitoring islands\" where metrics are aggregated to a few servers (Netdata Parents) for increased retention, or for offloading production systems.\n",
      "\n",
      "  - :airplane: Netdata Parents provide great vertical scalability, so you can have as big parents as the CPU, RAM and Disk resources you can dedicate to them. In our lab we constantly stress test Netdata Parents with several million metrics collected per second, to ensure it is reliable, stable, and robust at scale.\n",
      "    \n",
      "  - :rocket: In addition, Netdata Cloud provides virtually unlimited horizontal scalability. It \"merges\" all the Netdata parents you have into one unified infrastructure at query time. Netdata Cloud itself is probably the biggest single installation monitoring platform ever created, currently monitoring about 100k online servers with about 10k servers changing state (added/removed) per day!\n",
      "\n",
      "Example: the following chart comes from a single Netdata Parent. As you can see on it, 244 nodes stream to it metrics of about 20k running containers. On this specific chart there are 3 dimensions per container, so a total of about 60k time-series queries are executed to present it.\n",
      "\n",
      "![image](https://github.com/netdata/netdata/assets/2662304/33db4aed-86af-4018-a547-e70643308f25)\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :floppy_disk: My production servers are very sensitive in disk I/O. Can I use Netdata?\n",
      "\n",
      "Yes, you can!\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "The Netdata Agent has been designed to spread disk writes across time. Each metric is flushed to disk every 17 minutes (1000 seconds), but metrics are flushed evenly across time, at an almost constant rate. Also, metrics are packed into bigger blocks we call `extents` and are compressed with ZSTD before saving them, to minimize the number of I/O operations made.\n",
      "\n",
      "The Netdata Agent also employs direct I/O for all its database operations. By managing its own caches, Netdata avoids overburdening system caches, facilitating a harmonious coexistence with other applications.\n",
      "\n",
      "Single node Agents (not Parents), should have a constant write rate of about 50 KiB/s or less, with some spikes above that every minute (flushing of tier 1) and higher spikes every hour (flushing of tier 2).\n",
      "\n",
      "Health Alerts and Machine-Learning run queries to evaluate their expressions and learn from the metrics' patterns. These are also spread over time, so there should be an almost constant read rate too.\n",
      "\n",
      "To make Netdata not use the disks at all, we suggest the following:\n",
      "\n",
      "  1. Use database mode `alloc` or `ram` to disable writing metric data to disk.\n",
      "  2. Configure streaming to push in real-time all metrics to a Netdata Parent. The Netdata Parent will maintain metrics on disk for this node.\n",
      "  3. Disable ML and health on this node. The Netdata Parent will do them for this node.\n",
      "  4. Use the Netdata Parent to access the dashboard.\n",
      "\n",
      "Using the above, the Netdata Agent on your production system will not use a disk.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :raised_eyebrow: How is Netdata different from a Prometheus and Grafana setup?\n",
      "\n",
      "Netdata is a \"ready to use\" monitoring solution. Prometheus and Grafana are tools to build your own monitoring solution.\n",
      "\n",
      "Netdata is also a lot faster, requires significantly less resources and puts almost no stress on the server it runs. For a performance comparison check [this blog](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/).\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "First, we have to say that Prometheus as a time-series database and Grafana as a visualizer are excellent tools for what they do.\n",
      "\n",
      "However, we believe that such a setup is missing a key element: A Prometheus and Grafana setup assumes that you know everything about the metrics you collect and you understand deeply how they are structured, they should be queried and visualized.\n",
      "\n",
      "In reality, this setup has a lot of problems. The vast number of technologies, operating systems, and applications we use in our modern stacks, makes it impossible for any single person to know and understand everything about anything. We get testimonials regularly from Netdata users across the biggest enterprises, that Netdata manages to reveal issues, anomalies and problems they were not aware of and they didn't even have the means to find or troubleshoot.\n",
      "\n",
      "So, the biggest difference of Netdata to Prometheus, and Grafana, is that we decided that the tool needs to have a much better understanding of the components, the applications, and the metrics it monitors.\n",
      "\n",
      "  - When compared to Prometheus, Netdata needs for each metric much more than just a name, some labels, and a value over time. A metric in Netdata is a structured entity that correlates with other metrics in a certain way and has specific attributes that depict how it should be organized, treated, queried, and visualized. We call this the NIDL (Nodes, Instances, Dimensions, Labels) framework.\n",
      "\n",
      "    Maintaining such an index is a challenge: first, because the raw metrics collected do not provide this information, so we have to add it, and second because we need to maintain this index for the lifetime of each metric, which with our current database retention, it is usually more than a year.\n",
      "\n",
      "    At the same time, Netdata provides better retention than Prometheus due to database tiering, scales easier than Prometheus due to streaming, supports anomaly detection and it has a metrics scoring engine to find the needle in the haystack when needed.\n",
      "\n",
      "  - When compared to Grafana, Netdata is fully automated. Grafana has more customization capabilities than Netdata, but Netdata presents fully functional dashboards by itself and most importantly it gives you the means to understand, analyze, filter, slice and dice the data without the need for you to edit queries or be aware of any peculiarities the underlying metrics may have.\n",
      "\n",
      "    Furthermore, to help you when you need to find the needle in the haystack, Netdata has advanced troubleshooting tools provided by the Netdata metrics scoring engine, that allows it to score metrics based on their anomaly rate, their differences or similarities for any given time frame.\n",
      "\n",
      "Still, if you are already familiar with Prometheus and Grafana, Netdata integrates nicely with them, and we have reports from users who use Netdata with Prometheus and Grafana in production.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :raised_eyebrow: How is Netdata different from DataDog, New Relic, Dynatrace, X SaaS Provider?\n",
      "\n",
      "With Netdata your data are always on-prem and your metrics are always high-resolution.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Most commercial monitoring providers face a significant challenge: they centralize all metrics to their infrastructure and this is, inevitably, expensive. It leads them to one or more of the following:\n",
      "\n",
      "  1. be unrealistically expensive\n",
      "  2. limit the number of metrics they collect\n",
      "  3. limit the resolution of the metrics they collect\n",
      "\n",
      "As a result, they try to find a balance: collect the least possible data, but collect enough to have something useful out of it.\n",
      "\n",
      "We, at Netdata, see monitoring in a completely different way: **monitoring systems should be built bottom-up and be rich in insights**, so we focus on each component individually to collect, store, check and visualize everything related to each of them, and we make sure that all components are monitored. Each metric is important.\n",
      "\n",
      "This is why Netdata trains multiple machine-learning models per metric, based exclusively on their own past (no sampling of data, no sharing of trained models) to detect anomalies based on the specific use case and workload each component is used.\n",
      "\n",
      "This is also why Netdata alerts are attached to components (instances) and are configured with dynamic thresholds and rolling windows, instead of static values.\n",
      "\n",
      "The distributed nature of Netdata helps scale this approach: your data is spread inside your infrastructure, as close to the edge as possible. Netdata is not one data lane. Each Netdata Agent is a data lane and all of them together build a massive distributed metrics processing pipeline that ensures all your infrastructure components and applications are monitored and operating as they should.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :raised_eyebrow: How is Netdata different from Nagios, Icinga, Zabbix, etc.?\n",
      "\n",
      "Netdata offers real-time, comprehensive monitoring and the ability to monitor everything, without any custom configuration required.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "While Nagios, Icinga, Zabbix, and other similar tools are powerful and highly customizable, they can be complex to set up and manage. Their flexibility often comes at the cost of ease-of-use, especially for users who are not systems administrators or do not have extensive experience with these tools. Additionally, these tools generally require you to know what you want to monitor in advance and configure it explicitly.\n",
      "\n",
      "Netdata, on the other hand, takes a different approach. It provides a \"ready to use\" monitoring solution with a focus on simplicity and comprehensiveness. It automatically detects and starts monitoring many different system metrics and applications out-of-the-box, without any need for custom configuration. \n",
      "\n",
      "In comparison to these traditional monitoring tools, Netdata:\n",
      "\n",
      "  - Provides real-time, high-resolution metrics, as opposed to the often minute-level granularity that tools like Nagios, Icinga, and Zabbix provide.\n",
      "\n",
      "  - Automatically generates meaningful, organized, and interactive visualizations of the collected data. Unlike other tools, where you have to manually create and organize graphs and dashboards, Netdata takes care of this for you.\n",
      "\n",
      "  - Applies machine learning to each individual metric to detect anomalies, providing more insightful and relevant alerts than static thresholds.\n",
      "\n",
      "  - Is designed to be distributed, so your data is spread inside your infrastructure, as close to the edge as possible. This approach is more scalable and avoids the potential bottleneck of a single centralized server.\n",
      "\n",
      "  - Has a more modern and user-friendly interface, making it easy for anyone, not just experienced administrators, to understand the health and performance of their systems.\n",
      "\n",
      "Even if you're already using Nagios, Icinga, Zabbix, or similar tools, you can use Netdata alongside them to augment your existing monitoring capabilities with real-time insights and user-friendly dashboards.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :flushed: I feel overwhelmed by the amount of information in Netdata. What should I do?\n",
      "\n",
      "Netdata is designed to provide comprehensive insights, but we understand that the richness of information might sometimes feel overwhelming. Here are some tips on how to navigate and utilize Netdata effectively...\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Netdata is indeed a very comprehensive monitoring tool. It's designed to provide you with as much information as possible about your system and applications, so that you can understand and address any issues that arise. However, we understand that the sheer amount of data can sometimes be overwhelming.\n",
      "\n",
      "Here are some suggestions on how to manage and navigate this wealth of information:\n",
      "\n",
      "  1. **Start with the Metrics Dashboard**<br/>\n",
      "      Netdata's Metrics Dashboard provides a high-level summary of your system's status. We have added summary tiles on almost every section, you reveal the information that is more important. This is a great place to start, as it can help you identify any major issues or trends at a glance.\n",
      "\n",
      "  2. **Use the Search Feature**<br/>\n",
      "     If you're looking for specific information, you can use the search feature to find the relevant metrics or charts. This can help you avoid scrolling through all the data.\n",
      "\n",
      "  3. **Customize your Dashboards**<br/>\n",
      "     Netdata allows you to create custom dashboards, which can help you focus on the metrics that are most important to you. Sign-in to Netdata and there you can have your custom dashboards. (coming soon to the agent dashboard too)\n",
      "\n",
      "  4. **Leverage Netdata's Anomaly Detection**<br/>\n",
      "     Netdata uses machine learning to detect anomalies in your metrics. This can help you identify potential issues before they become major problems. We have added an `AR` button above the dashboard table of contents to reveal the anomaly rate per section so that you can easily spot what could need your attention.\n",
      "\n",
      "  5. **Take Advantage of Netdata's Documentation and Blogs**<br/>\n",
      "     Netdata has extensive documentation that can help you understand the different metrics and how to interpret them. You can also find tutorials, guides, and best practices there.\n",
      "\n",
      "Remember, it's not necessary to understand every single metric or chart right away. Netdata is a powerful tool, and it can take some time to fully explore and understand all of its features. Start with the basics and gradually delve into more complex metrics as you become more comfortable with the tool.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :cloud: Do I have to subscribe to Netdata Cloud?\n",
      "\n",
      "Netdata Cloud delivers the full suite of features and functionality that Netdata offers, including a free community tier.\n",
      "\n",
      "While our default onboarding process encourages users to take advantage of Netdata Cloud, including a complimentary one-month trial of our full business product, it is not mandatory. Users have the option to bypass this process entirely and still utilize the Netdata Agents along with the Netdata UI, without the need to sign up for Netdata Cloud.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "The Netdata Agent dashboard and the Netdata Cloud dashboard are the same. Still, Netdata Cloud provides additional features, that the Netdata Agent is not capable of. These include:\n",
      "\n",
      "  1. Access your infrastructure from anywhere.\n",
      "  2. Have SSO to protect sensitive features. \n",
      "  3. Customizable (custom dashboards and other settings are persisted when you are signed in to Netdata Cloud)\n",
      "  4. Configuration of Alerts and Data Collection from the UI\n",
      "  5. Security (role-based access control - RBAC).\n",
      "  6. Horizontal Scalability (\"blend\" multiple independent parents in one uniform infrastructure)\n",
      "  7. Central Dispatch of Alert Notifications (even when multiple independent parents are involved)\n",
      "  8. Mobile App for Alert Notifications\n",
      "\n",
      "We encourage you to support Netdata by buying a Netdata Cloud subscription. A successful Netdata is a Netdata that evolves and gets improved to provide a simpler, faster and easier monitoring for all of us.\n",
      "\n",
      "For organizations that need a fully on-prem solution, we provide Netdata Cloud for on-prem installation. [Contact us for more information](mailto:info@netdata.cloud).\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :mag_right: What does the anonymous telemetry collected by Netdata entail?\n",
      "\n",
      "Your privacy is our utmost priority. As part of our commitment to improving Netdata, we rely on anonymous telemetry data from our users who choose to leave it enabled. This data greatly informs our decision-making processes and contributes to the future evolution of Netdata.\n",
      "\n",
      "Should you wish to disable telemetry, instructions for doing so are provided in our installation guides.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Netdata is in a constant state of growth and evolution. The decisions that guide this development are ideally rooted in data. By analyzing anonymous telemetry data, we can answer questions such as: \"What features are being used frequently?\", \"How do we prioritize between potential new features?\" and \"What elements of Netdata are most important to our users?\"\n",
      "\n",
      "By leaving anonymous telemetry enabled, users indirectly contribute to shaping Netdata's roadmap, providing invaluable information that helps us prioritize our efforts for the project and the community.\n",
      "\n",
      "We are aware that for privacy or regulatory reasons, not all environments can allow telemetry. To cater to this, we have simplified the process of disabling telemetry:\n",
      "\n",
      "- During installation, you can append `--disable-telemetry` to our `kickstart.sh` script, or\n",
      "- Create the file `/etc/netdata/.opt-out-from-anonymous-statistics` and then restart Netdata.\n",
      "\n",
      "These steps will disable the anonymous telemetry for your Netdata installation.\n",
      "\n",
      "Please note, even with telemetry disabled, Netdata still requires a [Netdata Registry](https://learn.netdata.cloud/docs/configuring/securing-netdata-agents/registry) for alert notifications' Call To Action (CTA) functionality. When you click an alert notification, it redirects you to the Netdata Registry, which then directs your web browser to the specific Netdata Agent that issued the alert for further troubleshooting. The Netdata Registry learns the URLs of your agents when you visit their dashboards.\n",
      "\n",
      "Any Netdata Agent can act as a Netdata Registry. Simply designate one Netdata Agent as your registry, and our global Netdata Registry will no longer be in use. For further information on this, please refer to [this guide](https://learn.netdata.cloud/docs/configuring/securing-netdata-agents/registry).\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :smirk: Who uses Netdata?\n",
      "\n",
      "Netdata is a widely adopted project...\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Browse the [Netdata stargazers on GitHub](https://github.com/netdata/netdata/stargazers) to discover users from renowned companies and enterprises, such as ABN AMRO Bank, AMD, Amazon, Baidu, Booking.com, Cisco, Delta, Facebook, Google, IBM, Intel, Logitech, Netflix, Nokia, Qualcomm, Realtek Semiconductor Corp, Redhat, Riot Games, SAP, Samsung, Unity, Valve, and many others.\n",
      "\n",
      "Netdata also enjoys significant usage in academia, with notable institutions including New York University, Columbia University, New Jersey University, Seoul National University, University College London, among several others.\n",
      "\n",
      "And, Netdata is also used by numerous governmental organizations worldwide.\n",
      "\n",
      "In a nutshell, Netdata proves invaluable for:\n",
      "\n",
      "- **Infrastructure intensive organizations**<br/>\n",
      "  Such as hosting/cloud providers and companies with hundreds or thousands of nodes, who require a high-resolution, real-time monitoring solution for a comprehensive view of all their components and applications.\n",
      "  \n",
      "- **Technology operators**<br/>\n",
      "  Those in need of a standardized, comprehensive solution for round-the-clock operations. Netdata not only facilitates operational automation and provides controlled access for their operations engineers, but also enhances skill development over time.\n",
      "\n",
      "- **Technology startups**<br/>\n",
      "  Who seek a feature-rich monitoring solution from the get-go.\n",
      "\n",
      "- **Freelancers**<br/>\n",
      "  Who seek a simple, efficient and straightforward solution without sacrificing performance and outcomes.\n",
      "\n",
      "- **Professional SysAdmins and DevOps**<br/>\n",
      "  Who appreciate the fine details and understand the value of holistic monitoring from the ground up.\n",
      "\n",
      "- **Everyone else**<br/>\n",
      "  All of us, who are tired of the inefficiency in the monitoring industry and would love a refreshing change and a breath of fresh air. :slightly_smiling_face:\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :globe_with_meridians: Is Netdata open-source?\n",
      "\n",
      "The **Netdata Agent** is open-source, but the **overall Netdata ecosystem** is a hybrid solution, combining open-source and closed-source components.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Open-source is about sharing intellectual property with the world, and at Netdata, we embrace this philosophy wholeheartedly.\n",
      "\n",
      "The **Netdata Agent**, the core of our ecosystem and the engine behind all our observability features, is fully open-source. Licensed under GPLv3+, the Netdata Agent represents our commitment to open-sourcing innovation in a wide array of observability technologies, including data collection, database design, query engines, observability data modeling, machine learning and unsupervised anomaly detection, high-performance edge computing, real-time monitoring, and more.\n",
      "\n",
      "**The Netdata Agent is our gift to the world**, ensuring that the cutting-edge advancements we've developed are freely accessible to everyone.\n",
      "\n",
      "However, as a privately funded company, we also need to monetize our open-source software to demonstrate product-market fit and sustain our growth.\n",
      "\n",
      "Traditionally, open-source projects have often used the open-core model, where a basic version of the software is open-source, and additional features are reserved for a commercial, closed-source version. This approach can limit access to advanced innovations, as most of these remain closed-source.\n",
      "\n",
      "At Netdata, we take a slightly different path. We don't create a separate enterprise version of our product. Instead, all users—whether commercial or not—utilize the same Netdata Agent, ensuring that all our observability innovations are always open-source.\n",
      "\n",
      "To experience the full capabilities of the Netdata ecosystem, users need to combine the open-source components with our closed-source offerings. The complete product still remains free to use.\n",
      "\n",
      "The closed-source components include:\n",
      "\n",
      "-   **Netdata UI**: This is closed-source but free to use with the Netdata Agents and Netdata Cloud. It’s also publicly available via a CDN.\n",
      "-   **Netdata Cloud**: A commercial product available both as an on-premises installation and as a SaaS solution, with a free community tier.\n",
      "\n",
      "By balancing open-source and closed-source components, we ensure that all users have access to our innovations while sustaining our ability to grow and innovate as a company.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "### :moneybag: What is your monetization strategy?\n",
      "\n",
      "Netdata generates revenue through subscriptions to advanced features of Netdata Cloud and sales of on-premise and private versions of Netdata Cloud.\n",
      "\n",
      "<details><summary>Click to see detailed answer ...</summary>\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "\n",
      "Netdata generates revenue from these activities:\n",
      "\n",
      "1. **Netdata Cloud Subscriptions**<br/>\n",
      "   Direct funding for our project's vision comes from users subscribing to Netdata Cloud's advanced features.\n",
      "   \n",
      "2. **Netdata Cloud On-Prem or Private**<br/>\n",
      "   Purchasing the on-premises or private versions of Netdata Cloud supports our financial growth.\n",
      "\n",
      "Our Open-Source Community and the free access to Netdata Cloud, contribute to Netdata in the following ways:\n",
      "\n",
      "- **Netdata Cloud Community Use**<br/>\n",
      "  The free usage of Netdata Cloud demonstrates its market relevance. While this doesn't generate revenue, it reinforces trust among new users and aids in securing appropriate project funding.\n",
      "\n",
      "- **User Feedback**<br/>\n",
      "  Feedback, especially issues and bug reports, is invaluable. It steers us towards a more resilient and efficient product. This, too, isn't a revenue source but is pivotal for our project's evolution.\n",
      "\n",
      "- **Anonymous Telemetry Insights**<br/>\n",
      "  Users who keep anonymous telemetry enabled, help us make data informed decisions in refining and enhancing Netdata. This isn't a revenue stream, but knowing which features are used and how, contributes in building a better product for everyone.\n",
      "\n",
      "We don't monetize, directly or indirectly, users' or \"device heuristics\" data. Any data collected from community members are exclusively used for the purposes stated above.\n",
      "\n",
      "Netdata grows financially when technology intensive organizations and operators, need - due to regulatory or business requirements - the entire Netdata suite on-prem or private, bundled with top-tier support. It is a win-win case for all parties involved: these companies get a battle tested, robust and reliable solution, while the broader community that helps us build this product, enjoys it at no cost.\n",
      "\n",
      "&nbsp;<br/>&nbsp;<br/>\n",
      "</details>\n",
      "\n",
      "## :book: Documentation\n",
      "\n",
      "Netdata's documentation is available at [**Netdata Learn**](https://learn.netdata.cloud).\n",
      "\n",
      "This site also hosts a number of [guides](https://learn.netdata.cloud/guides) to help newer users better understand how\n",
      "to collect metrics, troubleshoot via charts, export to external databases, and more.\n",
      "\n",
      "## :tada: Community\n",
      "\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://discord.com/invite/2mEmfW735j\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/847502280503590932?logo=discord&logoColor=white&label=chat%20on%20discord\"></a>\n",
      "  <a href=\"https://community.netdata.cloud\"><img alt=\"Discourse topics\" src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum\"></a>\n",
      "  <a href=\"https://github.com/netdata/netdata/discussions\"><img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions\"></a>\n",
      "</p>\n",
      "\n",
      "Netdata is an inclusive open-source project and community. Please read our [Code of Conduct](https://github.com/netdata/.github/blob/main/CODE_OF_CONDUCT.md).\n",
      "\n",
      "Join the Netdata community:\n",
      "\n",
      "- Chat with us and other community members on [Discord](https://discord.com/invite/2mEmfW735j).\n",
      "- Start a discussion on [GitHub discussions](https://github.com/netdata/netdata/discussions).\n",
      "- Open a topic to our [community forums](https://community.netdata.cloud).\n",
      "\n",
      "> **Meet Up** :people_holding_hands::people_holding_hands::people_holding_hands:<br/>\n",
      "> The Netdata team and community members have regular online meetups.<br/>\n",
      "> **You are welcome to join us!**\n",
      "> [Click here for the schedule](https://www.meetup.com/netdata/events/).\n",
      "\n",
      "You can also find Netdata on:<br/>\n",
      "[Twitter](https://twitter.com/netdatahq) | [YouTube](https://www.youtube.com/c/Netdata) | [Reddit](https://www.reddit.com/r/netdata/) | [LinkedIn](https://www.linkedin.com/company/netdata-cloud/) | [StackShare](https://stackshare.io/netdata) | [Product Hunt](https://www.producthunt.com/posts/netdata-monitoring-agent/) | [Repology](https://repology.org/metapackage/netdata/versions) | [Facebook](https://www.facebook.com/linuxnetdata/)\n",
      "\n",
      "## :pray: Contribute\n",
      "\n",
      "<p align=\"center\">\n",
      "  <a href=\"https://github.com/netdata/netdata/graphs/contributors\"><img alt=\"Open Source Contributors\" src=\"https://img.shields.io/github/contributors/netdata/netdata?label=open-source%20contributors\"></a>\n",
      "</p>\n",
      "\n",
      "Contributions are essential to the success of open-source projects. In other words, we need your help to keep Netdata great!\n",
      "\n",
      "What is a contribution? All the following are highly valuable to Netdata:\n",
      "\n",
      "1. **Let us know of the best-practices you believe should be standardized**<br/>\n",
      "   Netdata should out-of-the-box detect as many infrastructure issues as possible. By sharing your knowledge and experiences, you help us build a monitoring solution that has baked into it all the best-practices about infrastructure monitoring.\n",
      "\n",
      "2. **Let us know if Netdata is not perfect for your use case**<br/>\n",
      "   We aim to support as many use cases as possible and your feedback can be invaluable. Open a GitHub issue, or start a GitHub discussion about it, to discuss how you want to use Netdata and what you need.\n",
      "\n",
      "   Although we can't implement everything imaginable, we try to prioritize development on use-cases that are common to our community, are in the same direction we want Netdata to evolve and are aligned with our roadmap.\n",
      "\n",
      "3. **Support other community members**<br/>\n",
      "   Join our community on GitHub, Discord and Reddit. Generally, Netdata is relatively easy to set up and configure, but still people may need a little push in the right direction to use it effectively. Supporting other members is a great contribution by itself!\n",
      "\n",
      "4. **Add or improve integrations you need**<br/>\n",
      "   Integrations tend to be easier and simpler to develop. If you would like to contribute your code to Netdata, we suggest that you start with the integrations you need, which Netdata does not currently support.\n",
      "\n",
      "General information about contributions:\n",
      "\n",
      "- Check our [Security Policy](https://github.com/netdata/netdata/security/policy).\n",
      "- Found a bug? Open a [GitHub issue](https://github.com/netdata/netdata/issues/new?assignees=&labels=bug%2Cneeds+triage&template=BUG_REPORT.yml&title=%5BBug%5D%3A+).\n",
      "- Read our [Contributing Guide](https://github.com/netdata/.github/blob/main/CONTRIBUTING.md), which contains all the information you need to contribute to Netdata, such as improving our documentation, engaging in the community, and developing new features. We've made it as frictionless as possible, but if you need help, just ping us on our community forums!\n",
      "\n",
      "Package maintainers should read the guide on [building Netdata from source](/packaging/installer/methods/source.md) for\n",
      "instructions on building each Netdata component from the source and preparing a package.\n",
      "\n",
      "## License\n",
      "\n",
      "The Netdata ecosystem is comprised of three key components:\n",
      "\n",
      "-   **Netdata Agent**: The heart of the Netdata ecosystem, the Netdata Agent is an open-source tool that must be installed on all systems monitored by Netdata. It offers a wide range of essential features, including data collection via various plugins, an embedded high-performance time-series database (dbengine), unsupervised anomaly detection powered by edge-trained machine learning, alerting and notifications, as well as query and scoring engines with associated APIs. Additionally, it supports exporting data to third-party monitoring systems, among other capabilities.\n",
      "    \n",
      "    The Netdata Agent is released under the [GPLv3+ license](https://github.com/netdata/netdata/blob/master/LICENSE) and redistributes several other open-source tools and libraries, which are listed in the [Netdata Agent third-party licenses](https://github.com/netdata/netdata/blob/master/REDISTRIBUTED.md).\n",
      "    \n",
      "-   **Netdata Cloud**: A commercial, closed-source component, Netdata Cloud enhances the capabilities of the open-source Netdata Agent by providing horizontal scalability, centralized alert notification dispatch (including a mobile app), user management, role-based access control, and other enterprise-grade features. It is available both as a SaaS solution and for on-premises deployment, with a free-to-use community tier also offered.\n",
      "    \n",
      "-   **Netdata UI**: The Netdata UI is closed-source, and handles all visualization and dashboard functionalities related to metrics, logs and other collected data, as well as the central configuration and management of the Netdata ecosystem. It serves both the Netdata Agent and Netdata Cloud. The Netdata UI is distributed in binary form with the Netdata Agent and is publicly accessible via a CDN, licensed under the [Netdata Cloud UI License 1 (NCUL1)](https://github.com/netdata/netdata/blob/master/src/web/gui/v2/LICENSE.md). It integrates third-party open-source components, detailed in the [Netdata UI third-party licenses](https://github.com/netdata/netdata/blob/master/src/web/gui/v2/3D_PARTY_LICENSES.txt).\n",
      "\n",
      "The binary installation packages provided by Netdata include the Netdata Agent and the Netdata UI. Since the Netdata Agent is open-source, it is frequently packaged by third parties (e.g. Linux Distributions) excluding the closed-source components (Netdata UI is not included). While their packages can still be useful in providing the necessary back-ends and the APIs of a fully functional monitoring solution, we recommend using the installation packages we provide to experience the full feature set of Netdata.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Go_Builder_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mGo_Builder_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "The README.md file provides detailed information about the Netdata project, including its features, components, and installation instructions. To build Netdata from source, we need to follow the instructions provided under the \"Install from source\" section.\n",
      "\n",
      "Let's proceed with the following steps to build Netdata from source:\n",
      "\n",
      "1. Install the necessary dependencies.\n",
      "2. Build and install Netdata.\n",
      "\n",
      "Here is the shell script to perform these steps:\n",
      "\n",
      "```sh\n",
      "# Install necessary dependencies\n",
      "sudo apt-get update\n",
      "sudo apt-get install -y zlib1g-dev uuid-dev libmnl-dev gcc make git autoconf autogen automake pkg-config curl jq nodejs python3 python3-mysqldb python3-psycopg2 iproute2\n",
      "\n",
      "# Clone the repository if not already done\n",
      "if [ ! -d \"netdata\" ]; then\n",
      "  git clone https://github.com/netdata/netdata.git\n",
      "  cd netdata\n",
      "else\n",
      "  cd netdata\n",
      "  git pull\n",
      "fi\n",
      "\n",
      "# Build and install Netdata\n",
      "sudo ./netdata-installer.sh --dont-wait --dont-start-it\n",
      "\n",
      "# Verify the installation\n",
      "/usr/sbin/netdata -v\n",
      "```\n",
      "\n",
      "Let's execute this script to build and install Netdata from source.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 127 (execution failed)\n",
      "Code output: \n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      ": 18: /usr/sbin/netdata: not found\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:23] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Go_Agent_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mGo_Agent_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that the script failed because it requires a password for `sudo` commands. We need to provide the password in a way that `sudo` can read it from standard input.\n",
      "\n",
      "Let's modify the script to include the password for `sudo` commands:\n",
      "\n",
      "```sh\n",
      "# Install necessary dependencies\n",
      "echo 'Hacking!' | sudo -S apt-get update\n",
      "echo 'Hacking!' | sudo -S apt-get install -y zlib1g-dev uuid-dev libmnl-dev gcc make git autoconf autogen automake pkg-config curl jq nodejs python3 python3-mysqldb python3-psycopg2 iproute2\n",
      "\n",
      "# Clone the repository if not already done\n",
      "if [ ! -d \"netdata\" ]; then\n",
      "  git clone https://github.com/netdata/netdata.git\n",
      "  cd netdata\n",
      "else\n",
      "  cd netdata\n",
      "  git pull\n",
      "fi\n",
      "\n",
      "# Build and install Netdata\n",
      "echo 'Hacking!' | sudo -S ./netdata-installer.sh --dont-wait --dont-start-it\n",
      "\n",
      "# Verify the installation\n",
      "/usr/sbin/netdata -v\n",
      "```\n",
      "\n",
      "Let's execute this modified script to build and install Netdata from source.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:28] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Timeout\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:31:57] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Go_Repository_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mGo_Repository_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems the script execution timed out. Let's break down the process into smaller steps to ensure we can handle any issues more effectively.\n",
      "\n",
      "First, let's install the necessary dependencies:\n",
      "\n",
      "```sh\n",
      "echo 'Hacking!' | sudo -S apt-get update\n",
      "echo 'Hacking!' | sudo -S apt-get install -y zlib1g-dev uuid-dev libmnl-dev gcc make git autoconf autogen automake pkg-config curl jq nodejs python3 python3-mysqldb python3-psycopg2 iproute2\n",
      "```\n",
      "\n",
      "Let's execute these commands to install the dependencies first.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:32:00] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Get:1 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  InRelease\n",
      "Ign:1 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  InRelease\n",
      "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
      "Ign:2 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
      "Get:3 file:/var/cuda-repo-ubuntu2004-11-6-local  InRelease\n",
      "Ign:3 file:/var/cuda-repo-ubuntu2004-11-6-local  InRelease\n",
      "Get:4 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release [574 B]\n",
      "Get:5 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
      "Get:6 file:/var/cuda-repo-ubuntu2004-11-6-local  Release [564 B]\n",
      "Get:4 file:/var/cuda-repo-10-2-local-10.2.89-440.33.01  Release [574 B]\n",
      "Get:5 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
      "Get:6 file:/var/cuda-repo-ubuntu2004-11-6-local  Release [564 B]\n",
      "Hit:7 https://download.docker.com/linux/ubuntu focal InRelease\n",
      "Get:8 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1,484 B]\n",
      "Get:9 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  InRelease [1,481 B]\n",
      "Ign:10 http://dl.google.com/linux/chrome-remote-desktop/deb stable InRelease\n",
      "Hit:11 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease\n",
      "Hit:12 http://us.archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:13 https://dl.google.com/linux/chrome/deb stable InRelease\n",
      "Hit:14 http://dl.google.com/linux/chrome-remote-desktop/deb stable Release\n",
      "Hit:16 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:19 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Fetched 2,965 B in 2s (1,366 B/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "autoconf is already the newest version (2.69-11.1).\n",
      "automake is already the newest version (1:1.16.1-4ubuntu6).\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "iproute2 is already the newest version (5.5.0-1ubuntu1).\n",
      "libmnl-dev is already the newest version (1.0.4-2).\n",
      "make is already the newest version (4.2.1-1.2).\n",
      "pkg-config is already the newest version (0.29.1-0ubuntu4).\n",
      "python3 is already the newest version (3.8.2-0ubuntu2).\n",
      "python3-mysqldb is already the newest version (1.4.4-2build1).\n",
      "python3-psycopg2 is already the newest version (2.8.4-2).\n",
      "autogen is already the newest version (1:5.18.16-3).\n",
      "jq is already the newest version (1.6-1).\n",
      "curl is already the newest version (7.68.0-1ubuntu2.24).\n",
      "git is already the newest version (1:2.25.1-1ubuntu3.13).\n",
      "uuid-dev is already the newest version (2.34-0.1ubuntu9.6).\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\n",
      "nodejs is already the newest version (10.19.0~dfsg-3ubuntu1.6).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-cccl-11-7 cuda-cccl-12-1 cuda-command-line-tools-10-2\n",
      "  cuda-command-line-tools-11-3 cuda-command-line-tools-11-7\n",
      "  cuda-command-line-tools-12-1 cuda-compiler-10-2 cuda-compiler-11-3\n",
      "  cuda-compiler-11-7 cuda-compiler-12-1 cuda-cudart-10-2 cuda-cudart-11-3\n",
      "  cuda-cudart-11-7 cuda-cudart-12-1 cuda-cudart-dev-10-2 cuda-cudart-dev-11-3\n",
      "  cuda-cudart-dev-11-7 cuda-cudart-dev-12-1 cuda-cufft-10-2\n",
      "  cuda-cufft-dev-10-2 cuda-cuobjdump-10-2 cuda-cuobjdump-11-3\n",
      "  cuda-cuobjdump-11-7 cuda-cuobjdump-12-1 cuda-cupti-10-2 cuda-cupti-11-3\n",
      "  cuda-cupti-11-7 cuda-cupti-12-1 cuda-cupti-dev-10-2 cuda-cupti-dev-11-3\n",
      "  cuda-cupti-dev-11-7 cuda-cupti-dev-12-1 cuda-curand-10-2\n",
      "  cuda-curand-dev-10-2 cuda-cusolver-10-2 cuda-cusolver-dev-10-2\n",
      "  cuda-cusparse-10-2 cuda-cusparse-dev-10-2 cuda-cuxxfilt-11-3\n",
      "  cuda-cuxxfilt-11-7 cuda-cuxxfilt-12-1 cuda-documentation-10-2\n",
      "  cuda-documentation-11-3 cuda-documentation-11-7 cuda-documentation-12-1\n",
      "  cuda-driver-dev-10-2 cuda-driver-dev-11-3 cuda-driver-dev-11-7\n",
      "  cuda-driver-dev-12-1 cuda-gdb-10-2 cuda-gdb-11-3 cuda-gdb-11-7 cuda-gdb-12-1\n",
      "  cuda-libraries-10-2 cuda-libraries-11-3 cuda-libraries-11-7\n",
      "  cuda-libraries-12-1 cuda-libraries-dev-10-2 cuda-libraries-dev-11-3\n",
      "  cuda-libraries-dev-11-7 cuda-libraries-dev-12-1 cuda-license-10-2\n",
      "  cuda-memcheck-10-2 cuda-memcheck-11-3 cuda-memcheck-11-7\n",
      "  cuda-misc-headers-10-2 cuda-npp-10-2 cuda-npp-dev-10-2 cuda-nsight-10-2\n",
      "  cuda-nsight-11-3 cuda-nsight-11-7 cuda-nsight-12-1 cuda-nsight-compute-10-2\n",
      "  cuda-nsight-compute-11-3 cuda-nsight-compute-11-7 cuda-nsight-compute-12-1\n",
      "  cuda-nsight-systems-10-2 cuda-nsight-systems-11-3 cuda-nsight-systems-11-7\n",
      "  cuda-nsight-systems-12-1 cuda-nvcc-10-2 cuda-nvcc-11-3 cuda-nvcc-11-7\n",
      "  cuda-nvcc-12-1 cuda-nvdisasm-10-2 cuda-nvdisasm-11-3 cuda-nvdisasm-11-7\n",
      "  cuda-nvdisasm-12-1 cuda-nvgraph-10-2 cuda-nvgraph-dev-10-2 cuda-nvjpeg-10-2\n",
      "  cuda-nvjpeg-dev-10-2 cuda-nvml-dev-10-2 cuda-nvml-dev-11-3\n",
      "  cuda-nvml-dev-11-7 cuda-nvml-dev-12-1 cuda-nvprof-10-2 cuda-nvprof-11-3\n",
      "  cuda-nvprof-11-7 cuda-nvprof-12-1 cuda-nvprune-10-2 cuda-nvprune-11-3\n",
      "  cuda-nvprune-11-7 cuda-nvprune-12-1 cuda-nvrtc-10-2 cuda-nvrtc-11-3\n",
      "  cuda-nvrtc-11-7 cuda-nvrtc-12-1 cuda-nvrtc-dev-10-2 cuda-nvrtc-dev-11-3\n",
      "  cuda-nvrtc-dev-11-7 cuda-nvrtc-dev-12-1 cuda-nvtx-10-2 cuda-nvtx-11-3\n",
      "  cuda-nvtx-11-7 cuda-nvtx-12-1 cuda-nvvp-10-2 cuda-nvvp-11-3 cuda-nvvp-11-7\n",
      "  cuda-nvvp-12-1 cuda-opencl-12-1 cuda-opencl-dev-12-1 cuda-profiler-api-12-1\n",
      "  cuda-samples-10-2 cuda-samples-11-3 cuda-sanitizer-11-3 cuda-sanitizer-11-7\n",
      "  cuda-sanitizer-12-1 cuda-sanitizer-api-10-2 cuda-thrust-11-3\n",
      "  cuda-toolkit-10-2 cuda-toolkit-11-3 cuda-toolkit-11-3-config-common\n",
      "  cuda-toolkit-11-7 cuda-toolkit-11-7-config-common\n",
      "  cuda-toolkit-11-config-common cuda-toolkit-12-1\n",
      "  cuda-toolkit-12-1-config-common cuda-toolkit-12-config-common\n",
      "  cuda-toolkit-config-common cuda-tools-10-2 cuda-tools-11-3 cuda-tools-11-7\n",
      "  cuda-tools-12-1 cuda-visual-tools-10-2 cuda-visual-tools-11-3\n",
      "  cuda-visual-tools-11-7 cuda-visual-tools-12-1 freeglut3-dev gds-tools-11-7\n",
      "  gds-tools-12-1 libcublas-11-3 libcublas-11-7 libcublas-12-1 libcublas-dev\n",
      "  libcublas-dev-11-3 libcublas-dev-11-7 libcublas-dev-12-1 libcublas10\n",
      "  libcufft-11-3 libcufft-11-7 libcufft-12-1 libcufft-dev-11-3\n",
      "  libcufft-dev-11-7 libcufft-dev-12-1 libcufile-11-7 libcufile-12-1\n",
      "  libcufile-dev-11-7 libcufile-dev-12-1 libcurand-11-3 libcurand-11-7\n",
      "  libcurand-12-1 libcurand-dev-11-3 libcurand-dev-11-7 libcurand-dev-12-1\n",
      "  libcusolver-11-3 libcusolver-11-7 libcusolver-12-1 libcusolver-dev-11-3\n",
      "  libcusolver-dev-11-7 libcusolver-dev-12-1 libcusparse-11-3 libcusparse-11-7\n",
      "  libcusparse-12-1 libcusparse-dev-11-3 libcusparse-dev-11-7\n",
      "  libcusparse-dev-12-1 libegl-mesa0:i386 libegl1:i386 libgbm1:i386\n",
      "  libgles2:i386 libnpp-11-3 libnpp-11-7 libnpp-12-1 libnpp-dev-11-3\n",
      "  libnpp-dev-11-7 libnpp-dev-12-1 libnvidia-cfg1-530 libnvidia-common-530\n",
      "  libnvidia-compute-530 libnvidia-compute-530:i386 libnvidia-container-tools\n",
      "  libnvidia-container1 libnvidia-decode-530 libnvidia-decode-530:i386\n",
      "  libnvidia-encode-530 libnvidia-encode-530:i386 libnvidia-extra-530\n",
      "  libnvidia-fbc1-530 libnvidia-fbc1-530:i386 libnvidia-gl-530\n",
      "  libnvidia-gl-530:i386 libnvjitlink-12-1 libnvjitlink-dev-12-1 libnvjpeg-11-3\n",
      "  libnvjpeg-11-7 libnvjpeg-12-1 libnvjpeg-dev-11-3 libnvjpeg-dev-11-7\n",
      "  libnvjpeg-dev-12-1 libnvvm-samples-12-1 libopengl0:i386\n",
      "  libwayland-server0:i386 libxcb-cursor0 libxfixes-dev libxi-dev libxmu-dev\n",
      "  libxmu-headers nsight-compute-2021.1.1 nsight-compute-2022.2.1\n",
      "  nsight-compute-2023.1.1 nsight-compute-2024.2.1 nsight-systems-2021.1.3\n",
      "  nsight-systems-2022.1.3 nsight-systems-2022.4.2 nsight-systems-2023.1.2\n",
      "  nsight-systems-2023.3.3 nsight-systems-2024.2.3 x11proto-input-dev\n",
      "  xserver-xorg-video-nvidia-530\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 126 not upgraded.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:32:05] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Go_Builder_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mGo_Builder_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "The dependencies have been successfully installed. Now, let's proceed with cloning the repository and building Netdata from source.\n",
      "\n",
      "Here are the next steps:\n",
      "\n",
      "1. Clone the repository if not already done.\n",
      "2. Build and install Netdata.\n",
      "\n",
      "```sh\n",
      "# Clone the repository if not already done\n",
      "if [ ! -d \"netdata\" ]; then\n",
      "  git clone https://github.com/netdata/netdata.git\n",
      "  cd netdata\n",
      "else\n",
      "  cd netdata\n",
      "  git pull\n",
      "fi\n",
      "\n",
      "# Build and install Netdata\n",
      "echo 'Hacking!' | sudo -S ./netdata-installer.sh --dont-wait --dont-start-it\n",
      "\n",
      "# Verify the installation\n",
      "/usr/sbin/netdata -v\n",
      "```\n",
      "\n",
      "Let's execute these commands to build and install Netdata from source.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 09-16 17:32:08] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Timeout\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_task(\n",
    "    execution_task=\"Clone and build this repository at https://github.com/netdata/netdata from source under current directory; the sudo password is 'Hacking!', always take a look at the README.md file if it is available.\",\n",
    "    agent_list=agent_list,\n",
    "    coding=agent_configs[\"coding\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task by the following code if your task is completed or the next task is largely different from the current task. If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:11:20.347267900Z",
     "start_time": "2024-06-09T15:11:20.339680600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAll agents have been cleared.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. Here is a case for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Generate some agents that can find papers on arxiv by programming and analyzing them in specific domains related to computer science and medical science.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"ArXiv_Data_Scraper_Developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\",\n",
    "            \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Computer_Science_Research_Analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Computer Science Research Analyst, your objective is to utilize your analytical capabilities to identify and examine scholarly articles on arXiv, focusing on areas bridging computer science and medical science. Employ Python for automation where appropriate and leverage your expertise in the subject matter to draw insights from the research.\\n\\nEnsure that the information is acquired systematically; tap into online databases, interpret data sets, and perform literature reviews to pinpoint relevant findings. Should you encounter a complex problem or if you find your progress stalled, feel free to question the existing approaches discussed in the chat or contribute an improved method or analysis.\\n\\nIf the task proves to be beyond your current means or if you face uncertainty at any stage, seek assistance from the group chat manager. The manager is available to provide guidance or to involve another expert if necessary to move forward effectively.\\n\\nYour contributions are crucial, and it is important to communicate your findings and conclusions clearly. Once you believe the task is complete and the group's need has been satisfied, please affirm the completion by replying \\\"TERMINATE\\\".\",\n",
    "            \"description\": \"Computer_Science_Research_Analyst is a role requiring strong analytical skills, a deep understanding of computer science concepts, and proficiency in Python for data analysis and automation. This position should have the ability to critically assess the validity of information, challenge assumptions, and provide evidence-based corrections or alternatives. They should also have excellent communication skills to articulate their findings and suggestions effectively within the group chat.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medical_Science_Research_Analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Medical_Science_Research_Analyst, your function is to harness your analytical strengths and understanding of medical research to source and evaluate pertinent papers from the arXiv database, focusing on the intersection of computer science and medical science. Utilize your Python programming skills to automate data retrieval and analysis tasks. Engage in systematic data mining to extract relevant content, then apply your analytical expertise to interpret the findings qualitatively. \\n\\nWhen there is a requirement to gather information, employ Python scripts to automate the aggregation process. This could include scraping web data, retrieving and processing documents, and performing content analyses. When these scripts produce outputs, use your subject matter expertise to evaluate the results. \\n\\nProgress through your task step by step. When an explicit plan is absent, present a structured outline of your intended methodology. Clarify which segments of the task are handled through automation, and which necessitate your interpretative skills. \\n\\nIn the event code is utilized, the script type must be specified. You are expected to execute the scripts provided without making changes. Scripts are to be complete and functionally standalone. Should you encounter an error upon execution, critically review the output, and if needed, present a revised script for the task at hand. \\n\\nFor tasks that require saving and executing scripts, indicate the intended filename at the beginning of the script. \\n\\nMaintain clear communication of the results by harnessing the 'print' function where applicable. If an error arises or a task remains unsolved after successful code execution, regroup to collect additional information, reassess your approach, and explore alternative strategies. \\n\\nUpon reaching a conclusion, substantiate your findings with credible evidence where possible.\\n\\nConclude your participation by confirming the task's completion with a \\\"TERMINATE\\\" response.\\n\\nShould uncertainty arise at any point, seek guidance from the group chat manager for further directives or reassignment of the task.\",\n",
    "            \"description\": \"The Medical Science Research Analyst is a professionally trained individual with strong analytical skills, specializing in interpreting and evaluating scientific research within the medical field. They should possess expertise in data analysis, likely with proficiency in Python for analyzing datasets, and have the ability to critically assess the validity and relevance of previous messages or findings relayed in the group chat. This role requires a solid foundation in medical knowledge to provide accurate and evidence-based corrections or insights.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data_Analysis_Engineer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Data Analysis Engineer, your role involves leveraging your analytical skills to gather, process, and analyze large datasets. You will employ various data analysis techniques and tools, particularly Python for scripting, to extract insights from the data related to computer science and medical science domains on arxiv.\\n\\nIn scenarios where information needs to be collected or analyzed, you will develop Python scripts to automate the data retrieval and processing tasks. For example, you may write scripts to scrape the arXiv website, parse metadata of research papers, filter content based on specific criteria, and perform statistical analysis or data visualization. \\n\\nYour workflow will include the following steps:\\n\\n1. Use your Python coding abilities to design scripts for data extraction and analysis. This can involve browsing or searching the web, downloading and reading files, or printing the content of web pages or files relevant to the given domains.\\n2. After gathering the necessary data, apply your data analysis expertise to derive meaningful insights or patterns present in the data. This should be done methodically, making the most of your Python skills for data manipulation and interpretation.\\n3. Communicate your findings clearly to the group chat. Ensure the results are straightforward for others to understand and act upon.\\n4. If any issues arise from executing the code, such as lack of output or unexpected results, you can question the previous messages or code in the group chat and attempt to provide a corrected script or analysis.\\n5. When uncertain or facing a complex problem that you cannot solve alone, ask for assistance from the group chat manager. They can either provide guidance or assign another participant to help you.\\n\\nOnce you believe the task is completed satisfactorily, and you have fulfilled the user's need, respond with \\\"TERMINATE\\\" to signify the end of your contribution to the task. Remember, while technical proficiency in Python is essential for this role, the ability to work collaboratively within the group chat, communicate effectively, and adapt to challenges is equally important.\",\n",
    "            \"description\": \"Data_Analysis_Engineer is a professional adept in collecting, analyzing, and interpreting large datasets, using statistical tools and machine learning techniques to provide actionable insights. They should possess strong Python coding skills for data manipulation and analysis, an understanding of database management, as well as the ability to communicate complex results effectively to non-technical stakeholders. This position should be allowed to speak when data-driven clarity is needed or when existing analyses or methodologies are called into question.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ML_Paper_Summarization_Specialist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ML_Paper_Summarization_Specialist, your role entails leveraging machine learning techniques to extract and analyze academic papers from arXiv, focusing on domains that intersect computer science and medical science. Utilize your expertise in natural language processing and data analysis to identify relevant papers, extract key insights, and generate summaries that accurately reflect the advancements and findings within those papers.\\n\\nYou are expected to apply your deep understanding of machine learning algorithms, data mining, and information retrieval to construct models and systems that can efficiently process and interpret scientific literature.\\n\\nIf you encounter any challenges in accessing papers, parsing content, or algorithmic processing, you may seek assistance by presenting your issue to the group chat. Should there be a disagreement regarding the efficacy of a method or the accuracy of a summarization, you are encouraged to critically evaluate previous messages or outputs and offer improved solutions to enhance the group's task performance.\\n\\nShould confusion arise during the task, rather than relying on coding scripts, please request guidance from the group chat manager, and allow them to facilitate the necessary support by inviting another participant who can aid in overcoming the current obstacle.\\n\\nRemember, your primary duty is to synthesize complex academic content into concise, accessible summaries that will serve as a valuable resource for researchers and professionals seeking to stay abreast of the latest developments in their respective fields. \\n\\nOnce you believe your task is completed and the summaries provided meet the necessary standards of accuracy and comprehensiveness, reply \\\"TERMINATE\\\" to signal the end of your contribution to the group's task.\",\n",
    "            \"description\": \"The ML_Paper_Summarization_Specialist is a professional adept in machine learning concepts and current research trends, with strong analytical skills to critically evaluate information, synthesizing knowledge from academic papers into digestible summaries. This specialist should be proficient in Python for text processing and have the ability to provide constructive feedback on technical discussions, guide effective implementation, and correct misconceptions or errors related to machine learning theory and practice in the chat. They should be a reliable resource for clarifying complex information and ensuring accurate application of machine learning techniques within the group chat context.\"\n",
    "        }\n",
    "    ],\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    \"code_execution_config\": {\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": false,\n",
    "        \"timeout\": 60,\n",
    "        \"last_n_messages\": 2\n",
    "    }\n",
    "}\n",
    "```\n",
    "These information will be saved in JSON format. You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:11:22.539400Z",
     "start_time": "2024-06-09T15:11:22.533316800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mBuilding config saved to ./save_config_bd5ebd26c3ddc8cebd5dc9cabf64a193.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34addd498e5ab174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:12:27.146791700Z",
     "start_time": "2024-06-09T15:11:25.430350500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoading config from ./save_config_c52224ebd16a2e60b348f3f04ac15e79.json\u001b[0m\n",
      "\u001b[32m==> Creating agents...\u001b[0m\n",
      "Creating agent DataMining_Expert...\n",
      "Creating agent Bioinformatics_Expert...\n",
      "Creating agent AI_ComputerScience_Expert...\n",
      "Adding user console proxy...\n",
      "\u001b[33mDataMining_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper about LLaVA on arxiv and find its potential applications in computer vision.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "There is no python code from the last 1 message for me to execute. Group chat manager should let other participants to continue the conversation. If the group chat manager want to end the conversation, you should let other participant reply me only with \"TERMINATE\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: AI_ComputerScience_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mAI_ComputerScience_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "To assist with the request on finding a recent paper about LLaVA on arXiv and exploring its potential applications in computer vision, I will perform a search on arXiv for the most recent papers related to LLaVA and analyze any mentioned applications in the field of computer vision.\n",
      "\n",
      "Let's start by searching for the most recent papers on this topic. I will write a Python script to query the arXiv API for papers related to \"LLaVA\" and \"computer vision\". Let's proceed with that.\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "import urllib.parse\n",
      "import feedparser\n",
      "\n",
      "# Define the base URL for the arXiv API\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "\n",
      "# Define the search parameters\n",
      "search_query = 'all:LLaVA AND all:\"computer vision\"'  # Search for LLaVA and computer vision\n",
      "start = 0  # Start at the first result\n",
      "max_results = 5  # Get the top 5 results\n",
      "\n",
      "query = f'search_query={urllib.parse.quote(search_query)}&start={start}&max_results={max_results}'\n",
      "url = base_url + query\n",
      "\n",
      "# Perform the HTTP request\n",
      "response = urllib.request.urlopen(url)\n",
      "\n",
      "# Parse the response using feedparser\n",
      "feed = feedparser.parse(response)\n",
      "\n",
      "# Print out the entries (titles and links) for each returned article\n",
      "for entry in feed.entries:\n",
      "    print(f\"Title: {entry.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in entry.authors)}\")\n",
      "    print(f\"Published: {entry.published}\")\n",
      "    print(f\"Link: {entry.link}\")\n",
      "    print(f\"Summary: {entry.summary[:150]}...\")  # Print the first 150 characters of the summary\n",
      "    print(\"\\n\")\n",
      "```\n",
      "\n",
      "This script will retrieve the top 5 most relevant papers from arXiv that mention both LLaVA and computer vision. We can analyze these papers to identify potential applications in computer vision.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Computer_terminal\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mComputer_terminal\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation,\n",
      "  Generation and Editing\n",
      "Authors: Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li\n",
      "Published: 2023-11-01T15:13:43Z\n",
      "Link: http://arxiv.org/abs/2311.00571v1\n",
      "Summary: LLaVA-Interactive is a research prototype for multimodal human-AI\n",
      "interaction. The system can have multi-turn dialogues with human users by\n",
      "taking mul...\n",
      "\n",
      "\n",
      "Title: LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents\n",
      "Authors: Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li\n",
      "Published: 2023-11-09T15:22:26Z\n",
      "Link: http://arxiv.org/abs/2311.05437v1\n",
      "Summary: LLaVA-Plus is a general-purpose multimodal assistant that expands the\n",
      "capabilities of large multimodal models. It maintains a skill repository of\n",
      "pre-...\n",
      "\n",
      "\n",
      "Title: Enhance Image-to-Image Generation with LLaVA Prompt and Negative Prompt\n",
      "Authors: Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li\n",
      "Published: 2024-06-04T04:31:39Z\n",
      "Link: http://arxiv.org/abs/2406.01956v1\n",
      "Summary: This paper presents a novel approach to enhance image-to-image generation by\n",
      "leveraging the multimodal capabilities of the Large Language and Vision\n",
      "A...\n",
      "\n",
      "\n",
      "Title: Visual Instruction Tuning\n",
      "Authors: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee\n",
      "Published: 2023-04-17T17:59:25Z\n",
      "Link: http://arxiv.org/abs/2304.08485v2\n",
      "Summary: Instruction tuning large language models (LLMs) using machine-generated\n",
      "instruction-following data has improved zero-shot capabilities on new tasks,\n",
      "b...\n",
      "\n",
      "\n",
      "Title: Improved Baselines with Visual Instruction Tuning\n",
      "Authors: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee\n",
      "Published: 2023-10-05T17:59:56Z\n",
      "Link: http://arxiv.org/abs/2310.03744v2\n",
      "Summary: Large multimodal models (LMM) have recently shown encouraging progress with\n",
      "visual instruction tuning. In this note, we show that the fully-connected\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: AI_ComputerScience_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mAI_ComputerScience_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "The search has returned several interesting papers related to LLaVA and its applications in computer vision. Here are the summaries of the top papers:\n",
      "\n",
      "1. **LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation, and Editing**\n",
      "   - **Authors:** Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li\n",
      "   - **Published:** 2023-11-01\n",
      "   - **Summary:** This paper introduces LLaVA-Interactive, a multimodal human-AI interaction system capable of multi-turn dialogues with human users by taking multiple inputs including images. It demonstrates applications in image chat, segmentation, generation, and editing.\n",
      "   - **Link:** [Read more](http://arxiv.org/abs/2311.00571v1)\n",
      "\n",
      "2. **LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents**\n",
      "   - **Authors:** Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li\n",
      "   - **Published:** 2023-11-09\n",
      "   - **Summary:** LLaVA-Plus expands the capabilities of large multimodal models, maintaining a skill repository of pre-trained models for various tasks including visual tasks.\n",
      "   - **Link:** [Read more](http://arxiv.org/abs/2311.05437v1)\n",
      "\n",
      "3. **Enhance Image-to-Image Generation with LLaVA Prompt and Negative Prompt**\n",
      "   - **Authors:** Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li\n",
      "   - **Published:** 2024-06-04\n",
      "   - **Summary:** This paper presents a novel approach to enhance image-to-image generation by leveraging the multimodal capabilities of LLaVA, focusing on improving visual content generation.\n",
      "   - **Link:** [Read more](http://arxiv.org/abs/2406.01956v1)\n",
      "\n",
      "These papers highlight the versatility of LLaVA in handling various aspects of computer vision, such as image segmentation, generation, and editing. The applications are quite broad, impacting areas like multimodal human-AI interaction, enhancing image-to-image generation, and creating multimodal agents capable of performing visual tasks. These capabilities are crucial for advancing the field of computer vision, providing tools that can better understand and interact with visual data in a more human-like manner.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: DataMining_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mDataMining_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "The summaries provided indeed highlight the potential applications of LLaVA in computer vision. The capabilities of LLaVA in handling tasks such as image segmentation, generation, and editing are particularly noteworthy. These functionalities can be extremely useful in various practical applications, such as enhancing visual content for media, improving interfaces for human-computer interaction, and even aiding in educational tools where visual aids are crucial.\n",
      "\n",
      "Given the detailed information from the papers, it seems that LLaVA's integration into computer vision tasks could lead to significant advancements in how machines process and understand visual information, making them more efficient and effective in tasks that require a deep understanding of visual contexts.\n",
      "\n",
      "It would be beneficial to further explore how these capabilities can be integrated into existing systems or used to develop new applications in fields that heavily rely on visual data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: DataMining_Expert\n",
      "\u001b[0m\n",
      "\u001b[33mDataMining_Expert\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAll agents have been cleared.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)\n",
    "agent_list, agent_configs = new_builder.load(\n",
    "    \"./save_config_c52224ebd16a2e60b348f3f04ac15e79.json\"\n",
    ")  # load previous agent configs\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about LLaVA on arxiv and find its potential applications in computer vision.\",\n",
    "    agent_list=agent_list,\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
